{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3362a434",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c50e74038bf488ad\n",
      "Reusing dataset csv (C:\\Users\\19148\\.cache\\huggingface\\datasets\\csv\\default-c50e74038bf488ad\\0.0.0\\9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(3664, ('去看看第一章映射就明白了，所谓反函数就是逆运算，比如说，那么，fyxfxy', 0))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_dataset(\"csv\",data_dir=\"C:/Users/19148/Documents/Pycharm_projects/paper_project/bert_test\", data_files=\"data_lite_big.csv\", split=split)\n",
    "        # self.dataset = load_dataset(\"csv\",\n",
    "        # data_dir=\"C:/Users/19148/Documents/Pycharm_projects/paper_project\", data_files=\"data_lite.csv\", split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "\n",
    "        return text, label\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "len(dataset), dataset[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70a58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59695a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/229\n",
      "2/229\n",
      "3/229\n",
      "4/229\n",
      "5/229\n",
      "6/229\n",
      "7/229\n",
      "8/229\n",
      "9/229\n",
      "10/229\n",
      "11/229\n",
      "12/229\n",
      "13/229\n",
      "14/229\n",
      "15/229\n",
      "16/229\n",
      "17/229\n",
      "18/229\n",
      "19/229\n",
      "20/229\n",
      "21/229\n",
      "22/229\n",
      "23/229\n",
      "24/229\n",
      "25/229\n",
      "26/229\n",
      "27/229\n",
      "28/229\n",
      "29/229\n",
      "30/229\n",
      "31/229\n",
      "32/229\n",
      "33/229\n",
      "34/229\n",
      "35/229\n",
      "36/229\n",
      "37/229\n",
      "38/229\n",
      "39/229\n",
      "40/229\n",
      "41/229\n",
      "42/229\n",
      "43/229\n",
      "44/229\n",
      "45/229\n",
      "46/229\n",
      "47/229\n",
      "48/229\n",
      "49/229\n",
      "50/229\n",
      "51/229\n",
      "52/229\n",
      "53/229\n",
      "54/229\n",
      "55/229\n",
      "56/229\n",
      "57/229\n",
      "58/229\n",
      "59/229\n",
      "60/229\n",
      "61/229\n",
      "62/229\n",
      "63/229\n",
      "64/229\n",
      "65/229\n",
      "66/229\n",
      "67/229\n",
      "68/229\n",
      "69/229\n",
      "70/229\n",
      "71/229\n",
      "72/229\n",
      "73/229\n",
      "74/229\n",
      "75/229\n",
      "76/229\n",
      "77/229\n",
      "78/229\n",
      "79/229\n",
      "80/229\n",
      "81/229\n",
      "82/229\n",
      "83/229\n",
      "84/229\n",
      "85/229\n",
      "86/229\n",
      "87/229\n",
      "88/229\n",
      "89/229\n",
      "90/229\n",
      "91/229\n",
      "92/229\n",
      "93/229\n",
      "94/229\n",
      "95/229\n",
      "96/229\n",
      "97/229\n",
      "98/229\n",
      "99/229\n",
      "100/229\n",
      "101/229\n",
      "102/229\n",
      "103/229\n",
      "104/229\n",
      "105/229\n",
      "106/229\n",
      "107/229\n",
      "108/229\n",
      "109/229\n",
      "110/229\n",
      "111/229\n",
      "112/229\n",
      "113/229\n",
      "114/229\n",
      "115/229\n",
      "116/229\n",
      "117/229\n",
      "118/229\n",
      "119/229\n",
      "120/229\n",
      "121/229\n",
      "122/229\n",
      "123/229\n",
      "124/229\n",
      "125/229\n",
      "126/229\n",
      "127/229\n",
      "128/229\n",
      "129/229\n",
      "130/229\n",
      "131/229\n",
      "132/229\n",
      "133/229\n",
      "134/229\n",
      "135/229\n",
      "136/229\n",
      "137/229\n",
      "138/229\n",
      "139/229\n",
      "140/229\n",
      "141/229\n",
      "142/229\n",
      "143/229\n",
      "144/229\n",
      "145/229\n",
      "146/229\n",
      "147/229\n",
      "148/229\n",
      "149/229\n",
      "150/229\n",
      "151/229\n",
      "152/229\n",
      "153/229\n",
      "154/229\n",
      "155/229\n",
      "156/229\n",
      "157/229\n",
      "158/229\n",
      "159/229\n",
      "160/229\n",
      "161/229\n",
      "162/229\n",
      "163/229\n",
      "164/229\n",
      "165/229\n",
      "166/229\n",
      "167/229\n",
      "168/229\n",
      "169/229\n",
      "170/229\n",
      "171/229\n",
      "172/229\n",
      "173/229\n",
      "174/229\n",
      "175/229\n",
      "176/229\n",
      "177/229\n",
      "178/229\n",
      "179/229\n",
      "180/229\n",
      "181/229\n",
      "182/229\n",
      "183/229\n",
      "184/229\n",
      "185/229\n",
      "186/229\n",
      "187/229\n",
      "188/229\n",
      "189/229\n",
      "190/229\n",
      "191/229\n",
      "192/229\n",
      "193/229\n",
      "194/229\n",
      "195/229\n",
      "196/229\n",
      "197/229\n",
      "198/229\n",
      "199/229\n",
      "200/229\n",
      "201/229\n",
      "202/229\n",
      "203/229\n",
      "204/229\n",
      "205/229\n",
      "206/229\n",
      "207/229\n",
      "208/229\n",
      "209/229\n",
      "210/229\n",
      "211/229\n",
      "212/229\n",
      "213/229\n",
      "214/229\n",
      "215/229\n",
      "216/229\n",
      "217/229\n",
      "218/229\n",
      "219/229\n",
      "220/229\n",
      "221/229\n",
      "222/229\n",
      "223/229\n",
      "224/229\n",
      "225/229\n",
      "226/229\n",
      "227/229\n",
      "228/229\n",
      "229/229\n",
      "229\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([16, 300]),\n torch.Size([16, 300]),\n torch.Size([16, 300]),\n tensor([0, 3, 1, 1, 0, 1, 3, 0, 0, 1, 1, 0, 1, 3, 0, 1]))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=300,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=False,\n",
    "                                     # shuffle=True,\n",
    "                                     drop_last=True)\n",
    "len_loader = len(loader)\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    print(\"%d/%d\"%(i+1,len_loader))\n",
    "    # print(labels)\n",
    "    # print(input_ids.shape)\n",
    "    # break\n",
    "\n",
    "print(len(loader))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f620d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 300, 768])\n",
      "1/229 2/229 3/229 4/229 5/229 6/229 7/229 8/229 \n",
      "9/229 10/229 11/229 12/229 13/229 14/229 15/229 16/229 \n",
      "17/229 18/229 19/229 20/229 21/229 22/229 23/229 24/229 \n",
      "25/229 26/229 27/229 28/229 29/229 30/229 31/229 32/229 \n",
      "33/229 34/229 35/229 36/229 37/229 38/229 39/229 40/229 \n",
      "41/229 42/229 43/229 44/229 45/229 46/229 47/229 48/229 \n",
      "49/229 50/229 51/229 52/229 53/229 54/229 55/229 56/229 \n",
      "57/229 58/229 59/229 60/229 61/229 62/229 63/229 64/229 \n",
      "65/229 66/229 67/229 68/229 69/229 70/229 71/229 72/229 \n",
      "73/229 74/229 75/229 76/229 77/229 78/229 79/229 80/229 \n",
      "81/229 82/229 83/229 84/229 85/229 86/229 87/229 88/229 \n",
      "89/229 90/229 91/229 92/229 93/229 94/229 95/229 96/229 \n",
      "97/229 98/229 99/229 100/229 101/229 102/229 103/229 104/229 \n",
      "105/229 106/229 107/229 108/229 109/229 110/229 111/229 112/229 \n",
      "113/229 114/229 115/229 116/229 117/229 118/229 119/229 120/229 \n",
      "121/229 122/229 123/229 124/229 125/229 126/229 127/229 128/229 \n",
      "129/229 130/229 131/229 132/229 133/229 134/229 135/229 136/229 \n",
      "137/229 138/229 139/229 140/229 141/229 142/229 143/229 144/229 \n",
      "145/229 146/229 147/229 148/229 149/229 150/229 151/229 152/229 \n",
      "153/229 154/229 155/229 156/229 157/229 158/229 159/229 160/229 \n",
      "161/229 162/229 163/229 164/229 165/229 166/229 167/229 168/229 \n",
      "169/229 170/229 171/229 172/229 173/229 174/229 175/229 176/229 \n",
      "177/229 178/229 179/229 180/229 181/229 182/229 183/229 184/229 \n",
      "185/229 186/229 187/229 188/229 189/229 190/229 191/229 192/229 \n",
      "193/229 194/229 195/229 196/229 197/229 198/229 199/229 200/229 \n",
      "201/229 202/229 203/229 204/229 205/229 206/229 207/229 208/229 \n",
      "209/229 210/229 211/229 212/229 213/229 214/229 215/229 216/229 \n",
      "217/229 218/229 219/229 220/229 221/229 222/229 223/229 224/229 \n",
      "225/229 226/229 227/229 228/229 229/229 torch.Size([3664, 768]) torch.Size([3664])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "#模型试算\n",
    "out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "\n",
    "print(out.last_hidden_state.shape)\n",
    "# print(out.last_hidden_state[1,0])\n",
    "w2v = torch.zeros(len(dataset), 768)\n",
    "label = torch.zeros(len(dataset))\n",
    "batch_size = 16\n",
    "len_loader = len(loader)\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader): # 这样就行了\n",
    "    print(\"%d/%d\"%(i+1,len_loader),end=' ')\n",
    "    if i%8 == 7:\n",
    "        print('\\n', end='')\n",
    "    out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "    vec = out.last_hidden_state\n",
    "    for j in range(batch_size):  # batch_size=16\n",
    "        # w2v[batch_size * i + j] = torch.mean(vec[j], dim=0, keepdim=True)  # 取均值，而不是第一个值\n",
    "        w2v[batch_size * i + j] = vec[j,0]  # vector\n",
    "        label[batch_size * i + j] = labels[j]\n",
    "print(w2v.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "w2v_list = w2v.tolist()\n",
    "# l_list = label.tolist()\n",
    "total = np.concatenate((w2v_list, label.reshape(len(label), 1)), axis=1)\n",
    "tp = pd.DataFrame(total)\n",
    "# tp.to_csv(\"w2v.csv\", header=None, index=None)\n",
    "tp.to_csv(\"w2v_big_mean.csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/229 2/229 3/229 4/229 5/229 6/229 7/229 8/229 \n",
      "9/229 10/229 11/229 12/229 13/229 14/229 15/229 16/229 \n",
      "17/229 18/229 19/229 20/229 21/229 22/229 23/229 24/229 \n",
      "25/229 26/229 27/229 28/229 29/229 30/229 31/229 32/229 \n",
      "33/229 34/229 35/229 36/229 37/229 38/229 39/229 40/229 \n",
      "41/229 42/229 43/229 44/229 45/229 46/229 47/229 48/229 \n",
      "49/229 50/229 51/229 52/229 53/229 54/229 55/229 56/229 \n",
      "57/229 58/229 59/229 60/229 61/229 62/229 63/229 64/229 \n",
      "65/229 66/229 67/229 68/229 69/229 70/229 71/229 72/229 \n",
      "73/229 74/229 75/229 76/229 77/229 78/229 79/229 80/229 \n",
      "81/229 82/229 83/229 84/229 85/229 86/229 87/229 88/229 \n",
      "89/229 90/229 91/229 92/229 93/229 94/229 95/229 96/229 \n",
      "97/229 98/229 99/229 100/229 101/229 102/229 103/229 104/229 \n",
      "105/229 106/229 107/229 108/229 109/229 110/229 111/229 112/229 \n",
      "113/229 114/229 115/229 116/229 117/229 118/229 119/229 120/229 \n",
      "121/229 122/229 123/229 124/229 125/229 126/229 127/229 128/229 \n",
      "129/229 130/229 131/229 132/229 133/229 134/229 135/229 136/229 \n",
      "137/229 138/229 139/229 140/229 141/229 142/229 143/229 144/229 \n",
      "145/229 146/229 147/229 148/229 149/229 150/229 151/229 152/229 \n",
      "153/229 154/229 155/229 156/229 157/229 158/229 159/229 160/229 \n",
      "161/229 162/229 163/229 164/229 165/229 166/229 167/229 168/229 \n",
      "169/229 170/229 171/229 172/229 173/229 174/229 175/229 176/229 \n",
      "177/229 178/229 179/229 180/229 181/229 182/229 183/229 184/229 \n",
      "185/229 186/229 187/229 188/229 189/229 190/229 191/229 192/229 \n",
      "193/229 194/229 195/229 196/229 197/229 198/229 199/229 200/229 \n",
      "201/229 202/229 203/229 204/229 205/229 206/229 207/229 208/229 \n",
      "209/229 210/229 211/229 212/229 213/229 214/229 215/229 216/229 \n",
      "217/229 218/229 219/229 220/229 221/229 222/229 223/229 224/229 \n",
      "225/229 226/229 227/229 228/229 229/229 torch.Size([3664, 300])\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 300)\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "        out = self.fc(out.last_hidden_state)\n",
    "        # out = out.softmax(dim=1)\n",
    "        return out\n",
    "model = Model()\n",
    "w2v_low_dim = torch.zeros(len(dataset), 300)\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader): # 这样就行了\n",
    "    print(\"%d/%d\"%(i+1,len_loader),end=' ')\n",
    "    if i%8 == 7:\n",
    "        print('\\n', end='')\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "    for j in range(batch_size):  # batch_size=16\n",
    "        # w2v_low_dim[batch_size * i + j] = out[j,0]  # vector\n",
    "        # 取均值，而不是第一个值\n",
    "        w2v_low_dim[batch_size * i + j] = torch.mean(out[j], dim=0, keepdim=True)\n",
    "print(w2v_low_dim.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3664, 300])\n",
      "torch.Size([3664, 300])\n"
     ]
    }
   ],
   "source": [
    "print(w2v_low_dim.shape)\n",
    "fc = torch.nn.Linear(768, 300)\n",
    "temp = fc(w2v)\n",
    "print(temp.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "w2v_low_dim_list = w2v_low_dim.tolist()\n",
    "# w2v_low_dim_list = temp.tolist()\n",
    "total_low_dim = np.concatenate((w2v_low_dim_list, label.reshape(len(label), 1)), axis=1)\n",
    "tq = pd.DataFrame(total_low_dim)\n",
    "# tp.to_csv(\"w2v.csv\", header=None, index=None)\n",
    "tq.to_csv(\"w2v_big_low_dim_mean.csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d3d02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义下游任务模型\n",
    "# class Model(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # self.fc = torch.nn.Linear(768, 2)\n",
    "#\n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "#         with torch.no_grad():\n",
    "#             out = pretrained(input_ids=input_ids,\n",
    "#                        attention_mask=attention_mask,\n",
    "#                        token_type_ids=token_type_ids)\n",
    "#\n",
    "#         out = self.fc(out.last_hidden_state)\n",
    "#\n",
    "#         # out = out.softmax(dim=1)\n",
    "#\n",
    "#         return out\n",
    "#\n",
    "#\n",
    "# model = Model()\n",
    "#\n",
    "# out = model(input_ids=input_ids,\n",
    "#       attention_mask=attention_mask,\n",
    "#       token_type_ids=token_type_ids)\n",
    "# print(out)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd44a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6782209873199463 0.625\n",
      "5 0.5840441584587097 0.75\n",
      "10 0.6991336941719055 0.375\n",
      "15 0.6459898948669434 0.625\n",
      "20 0.6601291298866272 0.5625\n",
      "25 0.6269813776016235 0.8125\n",
      "30 0.595515251159668 0.875\n",
      "35 0.5542500019073486 0.875\n",
      "40 0.5360901355743408 0.8125\n",
      "45 0.5258952379226685 0.9375\n",
      "50 0.5396168828010559 0.875\n",
      "55 0.5625669956207275 0.75\n",
      "60 0.503290057182312 0.875\n",
      "65 0.5144746899604797 0.75\n",
      "70 0.4966588318347931 0.875\n",
      "75 0.5100283622741699 0.8125\n",
      "80 0.5462980270385742 0.6875\n",
      "85 0.5015004873275757 0.9375\n",
      "90 0.4921759068965912 0.875\n",
      "95 0.5301690697669983 0.8125\n",
      "100 0.4322471618652344 0.9375\n",
      "105 0.4398854672908783 1.0\n",
      "110 0.6205551028251648 0.6875\n",
      "115 0.4555570185184479 0.9375\n",
      "120 0.43458104133605957 0.9375\n",
      "125 0.5856747031211853 0.8125\n",
      "130 0.45579853653907776 0.875\n",
      "135 0.49450209736824036 0.875\n",
      "140 0.4834059476852417 0.875\n",
      "145 0.41298091411590576 0.9375\n",
      "150 0.6239964365959167 0.6875\n",
      "155 0.42134153842926025 0.9375\n",
      "160 0.41760149598121643 1.0\n",
      "165 0.4275535047054291 0.9375\n",
      "170 0.5800575613975525 0.75\n",
      "175 0.44518887996673584 0.875\n",
      "180 0.42843857407569885 0.9375\n",
      "185 0.4298834800720215 0.9375\n",
      "190 0.46833470463752747 0.8125\n",
      "195 0.48308607935905457 0.8125\n",
      "200 0.5299521684646606 0.8125\n",
      "205 0.41276633739471436 0.9375\n",
      "210 0.4676920771598816 0.875\n",
      "215 0.4392228424549103 0.875\n",
      "220 0.4800220727920532 0.875\n",
      "225 0.5499932169914246 0.6875\n",
      "230 0.4250292479991913 0.875\n",
      "235 0.44226840138435364 0.9375\n",
      "240 0.527982771396637 0.75\n",
      "245 0.398252934217453 0.9375\n",
      "250 0.4963655173778534 0.875\n",
      "255 0.5123838186264038 0.8125\n",
      "260 0.4301964342594147 0.9375\n",
      "265 0.561985969543457 0.8125\n",
      "270 0.5266203880310059 0.75\n",
      "275 0.4799845516681671 0.8125\n",
      "280 0.3876492977142334 0.9375\n",
      "285 0.4564688503742218 0.8125\n",
      "290 0.4763532280921936 0.875\n",
      "295 0.538104772567749 0.75\n",
      "300 0.39595580101013184 0.9375\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "#训练\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# model.train()\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader): # 这样就行了\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275dd1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset chn_senti_corp (/Users/lee/.cache/huggingface/datasets/seamew___chn_senti_corp/default/0.0.0/1f242195a37831906957a11a2985a4329167e60657c07dc95ebe266c03fdfb85)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.86875\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}