{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset csv (C:\\Users\\19148\\.cache\\huggingface\\datasets\\csv\\default-896d6d854b3b0933\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2795 ('直接替换', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset csv (C:\\Users\\19148\\.cache\\huggingface\\datasets\\csv\\default-f4c04b1f5310396e\\0.0.0\\2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869 ('求助', 3)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset_train(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_dataset(\"csv\",data_dir=\"./\", data_files=\"data_lite_big_train.csv\", split=split)\n",
    "        # self.dataset = load_dataset(\"csv\",\n",
    "        # data_dir=\"C:/Users/19148/Documents/Pycharm_projects/paper_project\", data_files=\"data_lite.csv\", split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "\n",
    "        return text, label\n",
    "\n",
    "class Dataset_test(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_dataset(\"csv\",data_dir=\"./\", data_files=\"data_lite_big_test.csv\", split=split)\n",
    "        # self.dataset = load_dataset(\"csv\",\n",
    "        # data_dir=\"C:/Users/19148/Documents/Pycharm_projects/paper_project\", data_files=\"data_lite.csv\", split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "\n",
    "        return text, label\n",
    "\n",
    "dataset_train = Dataset_train('train')\n",
    "print(len(dataset_train), dataset_train[15])\n",
    "dataset_test = Dataset_test('train')\n",
    "print(len(dataset_test), dataset_test[15])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "token"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/186 2/186 3/186 4/186 5/186 6/186 7/186 8/186 \n",
      "9/186 10/186 11/186 12/186 13/186 14/186 15/186 16/186 \n",
      "17/186 18/186 19/186 20/186 21/186 22/186 23/186 24/186 \n",
      "25/186 26/186 27/186 28/186 29/186 30/186 31/186 32/186 \n",
      "33/186 34/186 35/186 36/186 37/186 38/186 39/186 40/186 \n",
      "41/186 42/186 43/186 44/186 45/186 46/186 47/186 48/186 \n",
      "49/186 50/186 51/186 52/186 53/186 54/186 55/186 56/186 \n",
      "57/186 58/186 59/186 60/186 61/186 62/186 63/186 64/186 \n",
      "65/186 66/186 67/186 68/186 69/186 70/186 71/186 72/186 \n",
      "73/186 74/186 75/186 76/186 77/186 78/186 79/186 80/186 \n",
      "81/186 82/186 83/186 84/186 85/186 86/186 87/186 88/186 \n",
      "89/186 90/186 91/186 92/186 93/186 94/186 95/186 96/186 \n",
      "97/186 98/186 99/186 100/186 101/186 102/186 103/186 104/186 \n",
      "105/186 106/186 107/186 108/186 109/186 110/186 111/186 112/186 \n",
      "113/186 114/186 115/186 116/186 117/186 118/186 119/186 120/186 \n",
      "121/186 122/186 123/186 124/186 125/186 126/186 127/186 128/186 \n",
      "129/186 130/186 131/186 132/186 133/186 134/186 135/186 136/186 \n",
      "137/186 138/186 139/186 140/186 141/186 142/186 143/186 144/186 \n",
      "145/186 146/186 147/186 148/186 149/186 150/186 151/186 152/186 \n",
      "153/186 154/186 155/186 156/186 157/186 158/186 159/186 160/186 \n",
      "161/186 162/186 163/186 164/186 165/186 166/186 167/186 168/186 \n",
      "169/186 170/186 171/186 172/186 173/186 174/186 175/186 176/186 \n",
      "177/186 178/186 179/186 180/186 181/186 182/186 183/186 184/186 \n",
      "185/186 186/186 186\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([15, 100]),\n torch.Size([15, 100]),\n torch.Size([15, 100]),\n tensor([0, 3, 3, 0, 2, 1, 1, 0, 2, 0, 3, 1, 2, 2, 0]))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=100,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "                                     batch_size=15,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     # shuffle=False,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "len_loader = len(loader)\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    print(\"%d/%d\"%(i+1,len_loader), end=' ')\n",
    "    if i%8 == 7: print('\\n', end='')\n",
    "    # print(labels)\n",
    "    # print(input_ids.shape)\n",
    "    # break\n",
    "\n",
    "print(len(loader))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "#模型试算\n",
    "out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "\n",
    "print(out.last_hidden_state.shape)\n",
    "# print(out.last_hidden_state[1,0])\n",
    "w2v = torch.zeros(len(dataset), 768)\n",
    "label = torch.zeros(len(dataset))\n",
    "batch_size = 16\n",
    "len_loader = len(loader)\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader): # 这样就行了\n",
    "    print(\"%d/%d\"%(i+1,len_loader),end=' ')\n",
    "    if i%8 == 7:\n",
    "        print('\\n', end='')\n",
    "    out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "    vec = out.last_hidden_state\n",
    "    for j in range(batch_size):  # batch_size=16\n",
    "        w2v[batch_size * i + j] = torch.mean(vec[j], dim=0, keepdim=True)  # 取均值，而不是第一个值\n",
    "        # w2v[batch_size * i + j] = vec[j,0]  # vector\n",
    "        label[batch_size * i + j] = labels[j]\n",
    "print(w2v.shape, label.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "w2v_list = w2v.tolist()\n",
    "# l_list = label.tolist()\n",
    "total = np.concatenate((w2v_list, label.reshape(len(label), 1)), axis=1)\n",
    "tp = pd.DataFrame(total)\n",
    "# tp.to_csv(\"w2v.csv\", header=None, index=None)\n",
    "tp.to_csv(\"w2v_big_mean.csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 300)\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "        out = self.fc(out.last_hidden_state)\n",
    "        # out = out.softmax(dim=1)\n",
    "        return out\n",
    "model = Model()\n",
    "w2v_low_dim = torch.zeros(len(dataset), 300)\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader): # 这样就行了\n",
    "    print(\"%d/%d\"%(i+1,len_loader),end=' ')\n",
    "    if i%8 == 7:\n",
    "        print('\\n', end='')\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "    for j in range(batch_size):  # batch_size=16\n",
    "        # w2v_low_dim[batch_size * i + j] = out[j,0]  # vector\n",
    "        # 取均值，而不是第一个值\n",
    "        w2v_low_dim[batch_size * i + j] = torch.mean(out[j], dim=0, keepdim=True)\n",
    "print(w2v_low_dim.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(w2v_low_dim.shape)\n",
    "fc = torch.nn.Linear(768, 300)\n",
    "temp = fc(w2v)\n",
    "print(temp.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "w2v_low_dim_list = w2v_low_dim.tolist()\n",
    "# w2v_low_dim_list = temp.tolist()\n",
    "total_low_dim = np.concatenate((w2v_low_dim_list, label.reshape(len(label), 1)), axis=1)\n",
    "tq = pd.DataFrame(total_low_dim)\n",
    "# tp.to_csv(\"w2v.csv\", header=None, index=None)\n",
    "tq.to_csv(\"w2v_big_low_dim_mean.csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class config(object):\n",
    "    '''\n",
    "    配置参数\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # 类别数\n",
    "        self.num_classes = 4\n",
    "\n",
    "        # 整体训练次数\n",
    "        self.num_epoch=3\n",
    "        # batch大小\n",
    "        self.batch_size=128\n",
    "        #每个序列最大token数\n",
    "        self.pad_size=32\n",
    "        #学习率\n",
    "        self.learning_rate = 1e-5\n",
    "\n",
    "        self.bert_path='bert_pretrain'\n",
    "        self.tokenizer=BertTokenizer.from_pretrained(self.bert_path) #定义分词器\n",
    "        self.hidden_size=768  # Bert模型 token的embedding维度 = Bert模型后接自定义分类器（单隐层全连接网络）的输入维度\n",
    "\n",
    "        # RNN 隐含层数量\n",
    "        self.rnn_hidden_size=256\n",
    "        # RNN数量\n",
    "        self.num_layers=256\n",
    "        # dropout\n",
    "        self.dropout=0.5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.2658, 0.2862, 0.2559],\n",
      "        [0.2190, 0.2734, 0.2465, 0.2611],\n",
      "        [0.2123, 0.2685, 0.2603, 0.2589],\n",
      "        [0.1912, 0.3117, 0.2410, 0.2562],\n",
      "        [0.2107, 0.2575, 0.2674, 0.2645],\n",
      "        [0.2027, 0.2731, 0.2630, 0.2612],\n",
      "        [0.1990, 0.2661, 0.2778, 0.2571],\n",
      "        [0.2297, 0.2618, 0.2617, 0.2467],\n",
      "        [0.2023, 0.2945, 0.2578, 0.2454],\n",
      "        [0.1919, 0.2682, 0.2598, 0.2801],\n",
      "        [0.2001, 0.2811, 0.2660, 0.2528],\n",
      "        [0.2168, 0.2898, 0.2388, 0.2547],\n",
      "        [0.2110, 0.2758, 0.2630, 0.2502]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0, 2, 2, 3, 2, 2, 3, 0, 2, 2, 0, 2, 1])\n",
      "tensor([2, 1, 1, 1, 2, 1, 2, 1, 1, 3, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([13, 4])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def standard_1(dim3_tensor):  # 归一化至-1 - 1\n",
    "    get_min = dim3_tensor.min(dim=2)[0].unsqueeze(2) #取最小值张量 索引舍弃\n",
    "    get_max = dim3_tensor.max(dim=2)[0].unsqueeze(2)\n",
    "    normed_tensor = ((dim3_tensor - get_min) / (get_max - get_min) - 0.5) / 2\n",
    "    return normed_tensor\n",
    "# 定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #加载预训练模型\n",
    "        self.fc_b = torch.nn.Linear(768, 768)\n",
    "        self.pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "        for param in self.pretrained.parameters():\n",
    "            param.requires_grad = True # 使参数可更新\n",
    "        # 在每个RNN神经元之间使用 dropout 在单个神经元内部的两个时间步长间不使用 dropout\n",
    "        self.lstm = torch.nn.LSTM(768,48,2,batch_first=True,bias=True,dropout=0.5)\n",
    "        # self.lstm = torch.nn.LSTM(768,100,1,batch_first=True,bias=True, bidirectional=True)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        # 双向LSTM要*2 分析LSTM节点数和网络层数时，看成神经元是LSTM全连接网络\n",
    "        self.fc=torch.nn.Linear(48,4) # 自定义全连接层 ，输入数（输入的最后一个维度），输出数（多分类数量），bert模型输出的最后一个维度是768，这里的输入要和bert最后的输出统一\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = self.pretrained(input_ids=input_ids,\n",
    "            attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        encoder, pooled = out.last_hidden_state, out.pooler_output\n",
    "        encoder = self.fc_b(encoder)  # 添加全连接层\n",
    "        encoder = self.fc_b(encoder)  # 添加全连接层\n",
    "        # print(encoder)\n",
    "        # encoder = standard_1(encoder)\n",
    "        # print(encoder)\n",
    "        # # bert output\n",
    "        # out = self.fc(encoder[:, 0])\n",
    "        # # out = out.last_hidden_state\n",
    "        # out = out.softmax(dim=1)\n",
    "        # print(encoder.shape, pooled.shape)\n",
    "\n",
    "        output, _ = self.lstm(encoder)  # lstm层\n",
    "        # output, _ = self.lstm(pooled)  # lstm层\n",
    "        output =self.dropout(output)\n",
    "        # print(output.shape)\n",
    "        output =output[:,-1,:]   #encoder#只要序列中最后一个token对应的输出，（因为lstm会记录前边token的信息）\n",
    "        output =self.fc(output)\n",
    "        output = output.softmax(dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "out = model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids)\n",
    "print(out)\n",
    "print(labels)\n",
    "pre = out.argmax(dim=1)\n",
    "print(pre)\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<function Tensor.type>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import BertModel\n",
    "# pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "# out = pretrained(input_ids=input_ids,\n",
    "#             attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "# encoder, pooled = out.last_hidden_state, out.pooler_output\n",
    "# lstm = torch.nn.LSTM(768,64,64,batch_first=True,dropout=0.5,bias=True,bidirectional=False)\n",
    "# output, _ = lstm(encoder)  # lstm层\n",
    "# output.type\n",
    "# # output.shape\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=2,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=False,\n",
    "                                     # shuffle=True,\n",
    "                                     drop_last=True)\n",
    "len_loader = len(loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3795267343521118 0.23076923076923078\n",
      "1 1.3397876024246216 0.5384615384615384\n",
      "2 1.3900090456008911 0.3076923076923077\n",
      "3 1.3097195625305176 0.6153846153846154\n",
      "4 1.3615437746047974 0.23076923076923078\n",
      "5 1.3342088460922241 0.46153846153846156\n",
      "6 1.2644776105880737 0.6153846153846154\n",
      "7 1.2953168153762817 0.38461538461538464\n",
      "8 1.3555538654327393 0.46153846153846156\n",
      "9 1.3034504652023315 0.46153846153846156\n",
      "10 1.3382264375686646 0.46153846153846156\n",
      "11 1.3957513570785522 0.23076923076923078\n",
      "12 1.3147149085998535 0.23076923076923078\n",
      "13 1.3067320585250854 0.46153846153846156\n",
      "14 1.3580595254898071 0.38461538461538464\n",
      "15 1.350270390510559 0.38461538461538464\n",
      "16 1.3052629232406616 0.38461538461538464\n",
      "17 1.3296529054641724 0.3076923076923077\n",
      "18 1.3284225463867188 0.3076923076923077\n",
      "19 1.2896473407745361 0.6153846153846154\n",
      "20 1.3403525352478027 0.46153846153846156\n",
      "21 1.2944469451904297 0.46153846153846156\n",
      "22 1.2755013704299927 0.5384615384615384\n",
      "23 1.3134956359863281 0.46153846153846156\n",
      "24 1.398621678352356 0.23076923076923078\n",
      "25 1.3759111166000366 0.23076923076923078\n",
      "26 1.3546397686004639 0.3076923076923077\n",
      "27 1.3117514848709106 0.38461538461538464\n",
      "28 1.285783052444458 0.46153846153846156\n",
      "29 1.2551305294036865 0.46153846153846156\n",
      "30 1.309591293334961 0.3076923076923077\n",
      "31 1.3063063621520996 0.38461538461538464\n",
      "32 1.3283556699752808 0.3076923076923077\n",
      "33 1.3542450666427612 0.3076923076923077\n",
      "34 1.2456421852111816 0.6923076923076923\n",
      "35 1.4026178121566772 0.23076923076923078\n",
      "36 1.291040301322937 0.6153846153846154\n",
      "37 1.36379075050354 0.15384615384615385\n",
      "38 1.3179574012756348 0.5384615384615384\n",
      "39 1.3419770002365112 0.3076923076923077\n",
      "40 1.360347032546997 0.23076923076923078\n",
      "41 1.3624763488769531 0.38461538461538464\n",
      "42 1.2742851972579956 0.46153846153846156\n",
      "43 1.278876543045044 0.46153846153846156\n",
      "44 1.3147698640823364 0.5384615384615384\n",
      "45 1.272972583770752 0.5384615384615384\n",
      "46 1.2749463319778442 0.5384615384615384\n",
      "47 1.2694193124771118 0.46153846153846156\n",
      "48 1.3737285137176514 0.23076923076923078\n",
      "49 1.3052397966384888 0.38461538461538464\n",
      "50 1.2952792644500732 0.5384615384615384\n",
      "51 1.3097208738327026 0.46153846153846156\n",
      "52 1.2468324899673462 0.6153846153846154\n",
      "53 1.25117826461792 0.5384615384615384\n",
      "54 1.3748855590820312 0.3076923076923077\n",
      "55 1.3531520366668701 0.23076923076923078\n",
      "56 1.2622865438461304 0.5384615384615384\n",
      "57 1.3217546939849854 0.46153846153846156\n",
      "58 1.2267961502075195 0.5384615384615384\n",
      "59 1.2665836811065674 0.5384615384615384\n",
      "60 1.2061197757720947 0.6923076923076923\n",
      "61 1.3170530796051025 0.5384615384615384\n",
      "62 1.2660523653030396 0.5384615384615384\n",
      "63 1.280387282371521 0.46153846153846156\n",
      "64 1.2561410665512085 0.5384615384615384\n",
      "65 1.3842101097106934 0.3076923076923077\n",
      "66 1.3992819786071777 0.23076923076923078\n",
      "67 1.3184459209442139 0.38461538461538464\n",
      "68 1.3169625997543335 0.46153846153846156\n",
      "69 1.3149878978729248 0.3076923076923077\n",
      "70 1.3151525259017944 0.38461538461538464\n",
      "71 1.313899278640747 0.38461538461538464\n",
      "72 1.3922022581100464 0.3076923076923077\n",
      "73 1.2714741230010986 0.46153846153846156\n",
      "74 1.2754192352294922 0.6153846153846154\n",
      "75 1.3494375944137573 0.3076923076923077\n",
      "76 1.2757138013839722 0.5384615384615384\n",
      "77 1.3054484128952026 0.38461538461538464\n",
      "78 1.2433452606201172 0.6923076923076923\n",
      "79 1.2657544612884521 0.46153846153846156\n",
      "80 1.233978271484375 0.6153846153846154\n",
      "81 1.3735913038253784 0.3076923076923077\n",
      "82 1.2632315158843994 0.46153846153846156\n",
      "83 1.2683207988739014 0.3076923076923077\n",
      "84 1.3130797147750854 0.3076923076923077\n",
      "85 1.3387118577957153 0.3076923076923077\n",
      "86 1.251568078994751 0.6153846153846154\n",
      "87 1.1779248714447021 0.6923076923076923\n",
      "88 1.3201197385787964 0.3076923076923077\n",
      "89 1.271934986114502 0.46153846153846156\n",
      "90 1.346474528312683 0.3076923076923077\n",
      "91 1.2868778705596924 0.3076923076923077\n",
      "92 1.2412078380584717 0.46153846153846156\n",
      "93 1.2022227048873901 0.5384615384615384\n",
      "94 1.1897962093353271 0.6153846153846154\n",
      "95 1.180135726928711 0.6923076923076923\n",
      "96 1.2454808950424194 0.6153846153846154\n",
      "97 1.2864614725112915 0.46153846153846156\n",
      "98 1.3487824201583862 0.38461538461538464\n",
      "99 1.3179789781570435 0.38461538461538464\n",
      "100 1.262668490409851 0.46153846153846156\n",
      "101 1.3626855611801147 0.38461538461538464\n",
      "102 1.1839035749435425 0.5384615384615384\n",
      "103 1.1760778427124023 0.5384615384615384\n",
      "104 1.2359657287597656 0.5384615384615384\n",
      "105 1.2488181591033936 0.5384615384615384\n",
      "106 1.1323741674423218 0.7692307692307693\n",
      "107 1.2868106365203857 0.46153846153846156\n",
      "108 1.1968039274215698 0.6923076923076923\n",
      "109 1.1867226362228394 0.7692307692307693\n",
      "110 1.258159875869751 0.6153846153846154\n",
      "111 1.2427293062210083 0.38461538461538464\n",
      "112 1.2081093788146973 0.6923076923076923\n",
      "113 1.370089054107666 0.3076923076923077\n",
      "114 1.1680991649627686 0.6923076923076923\n",
      "115 1.2380198240280151 0.5384615384615384\n",
      "116 1.3846237659454346 0.3076923076923077\n",
      "117 1.2618719339370728 0.46153846153846156\n",
      "118 1.2635924816131592 0.46153846153846156\n",
      "119 1.3086984157562256 0.46153846153846156\n",
      "120 1.2975386381149292 0.38461538461538464\n",
      "121 1.2595969438552856 0.5384615384615384\n",
      "122 1.196354627609253 0.8461538461538461\n",
      "123 1.2867388725280762 0.3076923076923077\n",
      "124 1.2958576679229736 0.46153846153846156\n",
      "125 1.221488118171692 0.5384615384615384\n",
      "126 1.2034772634506226 0.5384615384615384\n",
      "127 1.3045628070831299 0.38461538461538464\n",
      "128 1.2282488346099854 0.5384615384615384\n",
      "129 1.2678606510162354 0.5384615384615384\n",
      "130 1.2367749214172363 0.5384615384615384\n",
      "131 1.2293776273727417 0.6923076923076923\n",
      "132 1.2376145124435425 0.5384615384615384\n",
      "133 1.2563000917434692 0.5384615384615384\n",
      "134 1.1802027225494385 0.6153846153846154\n",
      "135 1.201779842376709 0.6923076923076923\n",
      "136 1.2628309726715088 0.46153846153846156\n",
      "137 1.2886720895767212 0.5384615384615384\n",
      "138 1.3074381351470947 0.46153846153846156\n",
      "139 1.358870267868042 0.3076923076923077\n",
      "140 1.3489271402359009 0.38461538461538464\n",
      "141 1.232523798942566 0.46153846153846156\n",
      "142 1.3241450786590576 0.38461538461538464\n",
      "143 1.254334807395935 0.6153846153846154\n",
      "144 1.276600956916809 0.5384615384615384\n",
      "145 1.2212636470794678 0.6153846153846154\n",
      "146 1.2993099689483643 0.46153846153846156\n",
      "147 1.3342088460922241 0.38461538461538464\n",
      "148 1.2235163450241089 0.46153846153846156\n",
      "149 1.3911627531051636 0.15384615384615385\n",
      "150 1.1800105571746826 0.6923076923076923\n",
      "151 1.267196774482727 0.38461538461538464\n",
      "152 1.1837108135223389 0.7692307692307693\n",
      "153 1.2680723667144775 0.46153846153846156\n",
      "154 1.323896884918213 0.38461538461538464\n",
      "155 1.2454694509506226 0.5384615384615384\n",
      "156 1.2085111141204834 0.6153846153846154\n",
      "157 1.247342824935913 0.46153846153846156\n",
      "158 1.2500542402267456 0.6153846153846154\n",
      "159 1.2655972242355347 0.46153846153846156\n",
      "160 1.315503478050232 0.46153846153846156\n",
      "161 1.2823007106781006 0.6153846153846154\n",
      "162 1.1755620241165161 0.7692307692307693\n",
      "163 1.258590579032898 0.38461538461538464\n",
      "164 1.192179799079895 0.6153846153846154\n",
      "165 1.1972320079803467 0.6923076923076923\n",
      "166 1.1827168464660645 0.6923076923076923\n",
      "167 1.2977054119110107 0.38461538461538464\n",
      "168 1.3223170042037964 0.3076923076923077\n",
      "169 1.2814208269119263 0.38461538461538464\n",
      "170 1.2258301973342896 0.5384615384615384\n",
      "171 1.3054555654525757 0.5384615384615384\n",
      "172 1.1659914255142212 0.5384615384615384\n",
      "173 1.292294979095459 0.38461538461538464\n",
      "174 1.2959105968475342 0.38461538461538464\n",
      "175 1.3106353282928467 0.38461538461538464\n",
      "176 1.2850830554962158 0.5384615384615384\n",
      "177 1.289651870727539 0.46153846153846156\n",
      "178 1.3119436502456665 0.46153846153846156\n",
      "179 1.2442244291305542 0.6153846153846154\n",
      "180 1.3309741020202637 0.46153846153846156\n",
      "181 1.289294958114624 0.46153846153846156\n",
      "182 1.2635951042175293 0.5384615384615384\n",
      "183 1.2213385105133057 0.6923076923076923\n",
      "184 1.3283543586730957 0.38461538461538464\n",
      "185 1.2814195156097412 0.5384615384615384\n",
      "186 1.1541482210159302 0.7692307692307693\n",
      "187 1.2865029573440552 0.38461538461538464\n",
      "188 1.253726840019226 0.5384615384615384\n",
      "189 1.3059298992156982 0.5384615384615384\n",
      "190 1.23722243309021 0.46153846153846156\n",
      "191 1.331179141998291 0.46153846153846156\n",
      "192 1.1971690654754639 0.6153846153846154\n",
      "193 1.1574469804763794 0.7692307692307693\n",
      "194 1.1552042961120605 0.7692307692307693\n",
      "195 1.1476205587387085 0.6923076923076923\n",
      "196 1.2354581356048584 0.5384615384615384\n",
      "197 1.1857883930206299 0.6923076923076923\n",
      "198 1.2741707563400269 0.5384615384615384\n",
      "199 1.277982473373413 0.38461538461538464\n",
      "200 1.3333913087844849 0.3076923076923077\n",
      "201 1.1796009540557861 0.6153846153846154\n",
      "202 1.3548957109451294 0.3076923076923077\n",
      "203 1.2833588123321533 0.5384615384615384\n",
      "204 1.261685848236084 0.6923076923076923\n",
      "205 1.209713101387024 0.5384615384615384\n",
      "206 1.3321049213409424 0.3076923076923077\n",
      "207 1.2010438442230225 0.5384615384615384\n",
      "208 1.2603989839553833 0.5384615384615384\n",
      "209 1.2318886518478394 0.5384615384615384\n",
      "210 1.1771206855773926 0.6923076923076923\n",
      "211 1.3027799129486084 0.38461538461538464\n",
      "212 1.2753658294677734 0.46153846153846156\n",
      "213 1.2665787935256958 0.46153846153846156\n",
      "214 1.2620667219161987 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "loader_train = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "                                     batch_size=13,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     # shuffle=False,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "#训练\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# model.train()\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader_train):\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 300:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 \n",
      "10 11 12 13 14 15 16 17 18 19 \n",
      "20 21 22 23 24 25 26 27 28 29 \n",
      "30 31 32 33 34 35 36 37 38 39 \n",
      "40 41 42 43 44 45 46 47 48 49 \n",
      "50 51 52 53 54 55 56 57 58 59 \n",
      "60 61 62 63 64 65 66 67 68 69 \n",
      "70 71 72 73 74 75 76 77 78 0.5788262370540852 503 869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6667    0.7234    0.6939       235\n",
      "         1.0     0.6543    0.6625    0.6584       160\n",
      "         2.0     0.4963    0.7625    0.6012       261\n",
      "         3.0     0.5490    0.1315    0.2121       213\n",
      "\n",
      "    accuracy                         0.5788       869\n",
      "   macro avg     0.5916    0.5700    0.5414       869\n",
      "weighted avg     0.5844    0.5788    0.5414       869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "from sklearn.metrics import classification_report\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    label = torch.zeros(len(dataset_test))\n",
    "    pre_label = torch.zeros(len(dataset_test))\n",
    "    batch_size=11\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=dataset_test,\n",
    "                                              batch_size=11,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "\n",
    "\n",
    "        print(i,end=' ')\n",
    "        if (i+1)%10 == 0 :\n",
    "            print('\\n',end='')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        # print(out)\n",
    "        for j in range(batch_size):  # batch_size=16\n",
    "            label[batch_size * i + j] = labels[j]\n",
    "            pre_label[batch_size * i + j] = out[j]\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total, correct, total)\n",
    "    print(classification_report(label,pre_label,digits=4))\n",
    "\n",
    "\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}