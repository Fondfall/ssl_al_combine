{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15454428  0.52875054 -0.47713432  0.26700583  0.5780986  -0.2584704\n",
      " -0.497395   -0.37840477 -0.04801681 -0.1354587  -0.30140558  0.3864191\n",
      "  0.10743058 -0.16617118 -0.04376784  0.19094634 -0.662355   -0.09070095\n",
      " -0.23156993 -0.5459718  -0.10523433 -0.20469089  0.09074577 -0.08418999\n",
      " -0.6287511  -0.4343539  -0.17811942 -0.26209912  0.59038424 -0.03774031\n",
      " -0.14668883  0.30349293  0.03785542 -0.2719773  -0.32126015 -0.06889755\n",
      "  0.03629145  0.33375648  0.01039888 -0.12757844  0.16452992 -0.07805916\n",
      "  0.6332627  -0.00853407  0.5796044  -0.4813189  -0.415879   -0.18914485\n",
      "  0.37880462 -0.535376    0.27505007 -0.0053158   0.43196863 -0.10982956\n",
      " -0.2483539   0.57755494  1.2750691  -0.2753227   0.21096511  0.17983645\n",
      "  0.07333843  0.32510185  0.21086293  0.35948765 -1.0670815  -0.24465425\n",
      " -0.05507161  0.22329137  0.18268651  0.23923065 -0.15428372  0.45899114\n",
      " -0.07298146 -0.14948614  0.28234842  0.2845556   0.48830715  0.0445042\n",
      "  0.2580861   0.5829236  -0.24267407 -0.1792513   0.06444535 -0.57681924\n",
      " -0.37758455 -0.48768583 -0.2026768  -0.13064437  0.0023602   0.08080367\n",
      "  0.35680678 -0.00684096 -0.21969618  0.02405959  0.20493968  0.24756914\n",
      "  0.3250612  -0.05901336 -0.573305   -0.32730082] \n",
      " 100\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练特征提取模型\n",
    "# reference website: https://wikipedia2vec.github.io/wikipedia2vec/usage/#api-usage\n",
    "import numpy as np\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "# 必须下载二进制文件否则下一步会报错，浪费我一个小时，怒😡\n",
    "wiki2vec = Wikipedia2Vec.load('zhwiki_20180420_100d.pkl')\n",
    "# test\n",
    "vec = wiki2vec.get_word_vector('我')\n",
    "vectorsize = vec.size\n",
    "print(vec,'\\n',vectorsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据总量: 2000 .\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                         text  label\n325                        谢谢，是我孤陋寡闻，洛必达法则没用过      0\n1375  函数的求导法则请老师写下图片中红框部分的演算步骤，以及下边蓝色划线部分怎么来的      2\n1692                                    求老师解答      3\n298                      x→∞，是x趋向于正无穷，还是趋向于无穷      0\n1171                                请问第二题怎么做啊      2\n1440                                   dy代表什么      2\n1283                               老师，为什么极限为啊      2\n1621                                      哪两个      3\n1735                                能不能写一下式子鸭      3\n1699                             老师请问这题该怎么解谢谢      3",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>325</th>\n      <td>谢谢，是我孤陋寡闻，洛必达法则没用过</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1375</th>\n      <td>函数的求导法则请老师写下图片中红框部分的演算步骤，以及下边蓝色划线部分怎么来的</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1692</th>\n      <td>求老师解答</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>x→∞，是x趋向于正无穷，还是趋向于无穷</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1171</th>\n      <td>请问第二题怎么做啊</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1440</th>\n      <td>dy代表什么</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1283</th>\n      <td>老师，为什么极限为啊</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1621</th>\n      <td>哪两个</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1735</th>\n      <td>能不能写一下式子鸭</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1699</th>\n      <td>老师请问这题该怎么解谢谢</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#定义删除除字母,数字，汉字以外的所有符号的函数\n",
    "def remove_punctuation(line):\n",
    "    line = str(line)\n",
    "    if line.strip()=='':\n",
    "        return ''\n",
    "    rule = re.compile(u\"[^a-zA-Z0-9\\u4E00-\\u9FA5]\")\n",
    "    line = rule.sub('',line)\n",
    "    return line\n",
    "\n",
    "#停用词列表\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "#加载停用词\n",
    "stopwords = stopwordslist(\"./stopwords.txt\")\n",
    "\n",
    "import pandas as pd\n",
    "# df = pd.read_csv('./data_lite_all.csv')\n",
    "df = pd.read_csv('./data_cleaned_1.csv')\n",
    "df=df[['text','label']]\n",
    "print(\"数据总量: %d .\" % len(df))\n",
    "df.sample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\WINDOWS\\TEMP\\jieba.cache\n",
      "Loading model cost 0.489 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                            text  label  \\\n0                                 为什么无界函数不一定为无穷大      0   \n1  判断，处有无定义，若，处有定义，但是，不存在，即，不存在或，不存在，若，处有定义存在，但是      0   \n2                                             谢谢      0   \n3                                     先开上面三次方再求导      0   \n4  求解答这里是讲收敛数列的有界性，为什么举例是这个例子这个数列并不收敛呀这个数列不是发散的吗      0   \n\n                                     clean_text  \\\n0                                为什么无界函数不一定为无穷大   \n1            判断处有无定义若处有定义但是不存在即不存在或不存在若处有定义存在但是   \n2                                            谢谢   \n3                                    先开上面三次方再求导   \n4  求解答这里是讲收敛数列的有界性为什么举例是这个例子这个数列并不收敛呀这个数列不是发散的吗   \n\n                                            cut_text  \n0                     [为什么, 无, 界, 函数, 不, 一定, 为, 无穷大]  \n1  [判断, 处, 有无, 定义, 若处, 有, 定义, 但是, 不, 存在, 即, 不, 存在...  \n2                                               [谢谢]  \n3                               [先开, 上面, 三次方, 再, 求导]  \n4  [求, 解答, 这里, 是, 讲, 收敛, 数列, 的, 有界性, 为什么, 举例, 是, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>clean_text</th>\n      <th>cut_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>为什么无界函数不一定为无穷大</td>\n      <td>0</td>\n      <td>为什么无界函数不一定为无穷大</td>\n      <td>[为什么, 无, 界, 函数, 不, 一定, 为, 无穷大]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>判断，处有无定义，若，处有定义，但是，不存在，即，不存在或，不存在，若，处有定义存在，但是</td>\n      <td>0</td>\n      <td>判断处有无定义若处有定义但是不存在即不存在或不存在若处有定义存在但是</td>\n      <td>[判断, 处, 有无, 定义, 若处, 有, 定义, 但是, 不, 存在, 即, 不, 存在...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>谢谢</td>\n      <td>0</td>\n      <td>谢谢</td>\n      <td>[谢谢]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>先开上面三次方再求导</td>\n      <td>0</td>\n      <td>先开上面三次方再求导</td>\n      <td>[先开, 上面, 三次方, 再, 求导]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>求解答这里是讲收敛数列的有界性，为什么举例是这个例子这个数列并不收敛呀这个数列不是发散的吗</td>\n      <td>0</td>\n      <td>求解答这里是讲收敛数列的有界性为什么举例是这个例子这个数列并不收敛呀这个数列不是发散的吗</td>\n      <td>[求, 解答, 这里, 是, 讲, 收敛, 数列, 的, 有界性, 为什么, 举例, 是, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba as jb\n",
    "#删除除字母,数字，汉字以外的所有符号\n",
    "df['clean_text'] = df['text'].apply(remove_punctuation)\n",
    "\n",
    "#分词，并过滤停用词\n",
    "# df['cut_review'] = df['clean_review'].apply(lambda x: \" \".join([w for w in list(jb.cut(x)) if w not in stopwords]))\n",
    "df['cut_text'] = df['clean_text'].apply(lambda x: [w for w in list(jb.cut(x)) if w not in stopwords])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "79                                     ([还有, 种, 方法], [0])\n344     ([函数, 的, 连续性, 麻烦, 老师, 看下例, 和, 例中, 画圈, 的, 部分, 怎...\n309            ([老师, 求解, 下面, 几个, 题目, 请, 老师, 解答, 一下], [0])\n1973                             ([能, 帮忙, 解, 一下, 吗], [3])\n1328                        ([请问, 这个, 极限, 怎么, 求, 啊], [2])\n1869    ([想要, 知道, 详细, 的, 过程, 和, 每, 一步, 的, 依据, 如图, 希望, ...\n1200                            ([老师, 这题, 怎么, 做, 啊], [2])\n1565                                   ([谁, 能, 帮帮我], [3])\n59                                      ([啊, 我会, 了], [0])\n1758                              ([老师, 这道题, 怎么, 做], [3])\ndtype: object"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "#创建标签化文档\n",
    "train_tagged = df.apply(\n",
    "    lambda r: TaggedDocument(words=r['cut_text'], tags=[r['label']]), axis=1)\n",
    "train_tagged.sample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "('无', [0], 0)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1=train_tagged[0][0]\n",
    "text2=train_tagged[0][1]\n",
    "text1[1],text2,text2[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([-0.2924,  0.1533, -0.1521,  0.4695,  0.1799,  0.1175, -0.1602, -0.7899,\n          0.7427, -0.0958,  0.0947,  0.2020, -0.4102, -0.1616,  0.1495, -0.2397,\n         -0.2724,  0.0600, -0.0103, -0.1817, -0.1020,  0.0334,  0.2580, -0.1205,\n         -0.5100, -0.3163, -0.0271, -0.1161,  0.2077, -0.1881,  0.2675,  0.2193,\n          0.5156,  0.3702, -0.0327, -0.1584,  0.3373, -0.1080, -0.0819, -0.1874,\n          0.1980,  0.1727,  0.3929, -0.3468,  0.5301, -0.4854, -0.5363, -0.2199,\n          0.1926, -0.6094,  0.0602,  0.6900,  0.4246,  0.4248,  0.2148, -0.0471,\n          0.8418, -0.2958,  0.3884,  0.5980,  0.3890,  0.3865, -0.3905, -0.1531,\n         -0.7449, -0.0657,  0.0027,  0.4643,  0.1590, -0.1030,  0.1285,  0.5606,\n          0.3592, -0.1796, -0.2919,  0.0113,  0.3375, -0.1267,  0.2550,  0.5555,\n         -0.1158, -0.1533, -0.0063, -0.5107, -0.0514, -0.2981, -0.2564,  0.0921,\n         -0.1424, -0.5418,  0.2905, -0.2002, -0.1603,  0.9346,  0.0510,  0.0556,\n          0.5027,  0.0431, -0.3708, -0.9284]),\n tensor([0., 0., 0.,  ..., 3., 3., 3.]))"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# 分词后查阅预训练模型中每个单词的特征，然后取平均值，为避免词典中没有某个词而导致查询时整个程序停止，引入系统中\n",
    "# 初始化数据\n",
    "data_length = 2000\n",
    "word_vector_size = 100\n",
    "label = torch.zeros(data_length)\n",
    "features = torch.zeros(data_length,word_vector_size)\n",
    "for i in range(len(train_tagged)):\n",
    "    cut_text = train_tagged[i][0]  # 分词后文本\n",
    "    label[i] = train_tagged[i][1][0]  # 标签存入label\n",
    "    # 每个词的词向量\n",
    "    count = 0\n",
    "    for j in range(len(cut_text)):\n",
    "        try:\n",
    "            vec_temp = wiki2vec.get_word_vector(cut_text[j]) # 获取当前词的词向量\n",
    "            features[i] += vec_temp\n",
    "            # print(vec_temp)\n",
    "            count += 1\n",
    "        except Exception as f:\n",
    "            # print(\"Exception f\")\n",
    "            ex = 1\n",
    "    if count!=0: features[i] /= count\n",
    "    # print(features[i])\n",
    "    # break\n",
    "features[1999],label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.01 0.3625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9231    0.2400    0.3810       100\n",
      "         1.0     0.7027    0.2549    0.3741       102\n",
      "         2.0     0.2819    0.9694    0.4368        98\n",
      "         3.0     0.0000    0.0000    0.0000       100\n",
      "\n",
      "    accuracy                         0.3625       400\n",
      "   macro avg     0.4769    0.3661    0.2980       400\n",
      "weighted avg     0.4790    0.3625    0.2976       400\n",
      "\n",
      "1 0.010974987654930561 0.3725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9259    0.2500    0.3937       100\n",
      "         1.0     0.6829    0.2745    0.3916       102\n",
      "         2.0     0.2857    0.9592    0.4403        98\n",
      "         3.0     0.6667    0.0200    0.0388       100\n",
      "\n",
      "    accuracy                         0.3725       400\n",
      "   macro avg     0.6403    0.3759    0.3161       400\n",
      "weighted avg     0.6423    0.3725    0.3159       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.012045035402587823 0.3875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9259    0.2500    0.3937       100\n",
      "         1.0     0.6939    0.3333    0.4503       102\n",
      "         2.0     0.2928    0.9592    0.4487        98\n",
      "         3.0     0.6667    0.0200    0.0388       100\n",
      "\n",
      "    accuracy                         0.3875       400\n",
      "   macro avg     0.6448    0.3906    0.3329       400\n",
      "weighted avg     0.6468    0.3875    0.3329       400\n",
      "\n",
      "3 0.013219411484660288 0.395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9286    0.2600    0.4062       100\n",
      "         1.0     0.6981    0.3627    0.4774       102\n",
      "         2.0     0.2943    0.9490    0.4493        98\n",
      "         3.0     0.6667    0.0200    0.0388       100\n",
      "\n",
      "    accuracy                         0.3950       400\n",
      "   macro avg     0.6469    0.3979    0.3429       400\n",
      "weighted avg     0.6489    0.3950    0.3431       400\n",
      "\n",
      "4 0.014508287784959394 0.4025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9310    0.2700    0.4186       100\n",
      "         1.0     0.6964    0.3824    0.4937       102\n",
      "         2.0     0.2981    0.9490    0.4537        98\n",
      "         3.0     0.6667    0.0200    0.0388       100\n",
      "\n",
      "    accuracy                         0.4025       400\n",
      "   macro avg     0.6481    0.4053    0.3512       400\n",
      "weighted avg     0.6500    0.4025    0.3514       400\n",
      "\n",
      "5 0.015922827933410922 0.415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9355    0.2900    0.4427       100\n",
      "         1.0     0.7119    0.4118    0.5217       102\n",
      "         2.0     0.3029    0.9490    0.4593        98\n",
      "         3.0     0.6667    0.0200    0.0388       100\n",
      "\n",
      "    accuracy                         0.4150       400\n",
      "   macro avg     0.6542    0.4177    0.3656       400\n",
      "weighted avg     0.6563    0.4150    0.3660       400\n",
      "\n",
      "6 0.01747528400007684 0.4275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9375    0.3000    0.4545       100\n",
      "         1.0     0.7302    0.4510    0.5576       102\n",
      "         2.0     0.3079    0.9490    0.4650        98\n",
      "         3.0     0.6667    0.0200    0.0388       100\n",
      "\n",
      "    accuracy                         0.4275       400\n",
      "   macro avg     0.6606    0.4300    0.3790       400\n",
      "weighted avg     0.6627    0.4275    0.3795       400\n",
      "\n",
      "7 0.019179102616724886 0.4475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9429    0.3300    0.4889       100\n",
      "         1.0     0.7286    0.5000    0.5930       102\n",
      "         2.0     0.3162    0.9388    0.4730        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4475       400\n",
      "   macro avg     0.6844    0.4497    0.4032       400\n",
      "weighted avg     0.6865    0.4475    0.4038       400\n",
      "\n",
      "8 0.02104904144512021 0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9444    0.3400    0.5000       100\n",
      "         1.0     0.7237    0.5392    0.6180       102\n",
      "         2.0     0.3239    0.9388    0.4817        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4600       400\n",
      "   macro avg     0.6855    0.4620    0.4143       400\n",
      "weighted avg     0.6875    0.4600    0.4150       400\n",
      "\n",
      "9 0.023101297000831605 0.4675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9444    0.3400    0.5000       100\n",
      "         1.0     0.7073    0.5686    0.6304       102\n",
      "         2.0     0.3309    0.9388    0.4894        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4675       400\n",
      "   macro avg     0.6832    0.4694    0.4194       400\n",
      "weighted avg     0.6851    0.4675    0.4201       400\n",
      "\n",
      "10 0.025353644939701114 0.4725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9444    0.3400    0.5000       100\n",
      "         1.0     0.7059    0.5882    0.6417       102\n",
      "         2.0     0.3345    0.9388    0.4933        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4725       400\n",
      "   macro avg     0.6837    0.4743    0.4232       400\n",
      "weighted avg     0.6856    0.4725    0.4239       400\n",
      "\n",
      "11 0.027825594022071243 0.4825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9444    0.3400    0.5000       100\n",
      "         1.0     0.7111    0.6275    0.6667       102\n",
      "         2.0     0.3407    0.9388    0.5000        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4825       400\n",
      "   macro avg     0.6866    0.4841    0.4311       400\n",
      "weighted avg     0.6884    0.4825    0.4319       400\n",
      "\n",
      "12 0.030538555088334154 0.4875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9459    0.3500    0.5109       100\n",
      "         1.0     0.7143    0.6373    0.6736       102\n",
      "         2.0     0.3433    0.9388    0.5027        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4875       400\n",
      "   macro avg     0.6884    0.4890    0.4362       400\n",
      "weighted avg     0.6902    0.4875    0.4371       400\n",
      "\n",
      "13 0.033516026509388425 0.4875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9459    0.3500    0.5109       100\n",
      "         1.0     0.6989    0.6373    0.6667       102\n",
      "         2.0     0.3459    0.9388    0.5055        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4875       400\n",
      "   macro avg     0.6852    0.4890    0.4352       400\n",
      "weighted avg     0.6869    0.4875    0.4360       400\n",
      "\n",
      "14 0.03678379771828634 0.4925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6804    0.6471    0.6633       102\n",
      "         2.0     0.3500    0.9286    0.5084        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4925       400\n",
      "   macro avg     0.6823    0.4939    0.4404       400\n",
      "weighted avg     0.6839    0.4925    0.4412       400\n",
      "\n",
      "15 0.040370172585965536 0.4925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6768    0.6569    0.6667       102\n",
      "         2.0     0.3488    0.9184    0.5056        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4925       400\n",
      "   macro avg     0.6811    0.4938    0.4406       400\n",
      "weighted avg     0.6827    0.4925    0.4414       400\n",
      "\n",
      "16 0.044306214575838825 0.4925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6667    0.6667    0.6667       102\n",
      "         2.0     0.3490    0.9082    0.5042        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4925       400\n",
      "   macro avg     0.6786    0.4937    0.4402       400\n",
      "weighted avg     0.6802    0.4925    0.4411       400\n",
      "\n",
      "17 0.04862601580065353 0.495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6509    0.6765    0.6635       102\n",
      "         2.0     0.3546    0.9082    0.5100        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4950       400\n",
      "   macro avg     0.6761    0.4962    0.4409       400\n",
      "weighted avg     0.6775    0.4950    0.4417       400\n",
      "\n",
      "18 0.0533669923120631 0.4975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6542    0.6863    0.6699       102\n",
      "         2.0     0.3560    0.9082    0.5115        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4975       400\n",
      "   macro avg     0.6772    0.4986    0.4429       400\n",
      "weighted avg     0.6787    0.4975    0.4436       400\n",
      "\n",
      "19 0.05857020818056667 0.4975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6514    0.6961    0.6730       102\n",
      "         2.0     0.3548    0.8980    0.5087        98\n",
      "         3.0     0.7500    0.0300    0.0577       100\n",
      "\n",
      "    accuracy                         0.4975       400\n",
      "   macro avg     0.6762    0.4985    0.4429       400\n",
      "weighted avg     0.6777    0.4975    0.4438       400\n",
      "\n",
      "20 0.06428073117284322 0.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6893    0.6961    0.6927       102\n",
      "         2.0     0.3548    0.8980    0.5087        98\n",
      "         3.0     0.8000    0.0800    0.1455       100\n",
      "\n",
      "    accuracy                         0.5100       400\n",
      "   macro avg     0.6982    0.5110    0.4698       400\n",
      "weighted avg     0.6999    0.5100    0.4707       400\n",
      "\n",
      "21 0.07054802310718646 0.5125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6792    0.7059    0.6923       102\n",
      "         2.0     0.3592    0.8980    0.5131        98\n",
      "         3.0     0.8000    0.0800    0.1455       100\n",
      "\n",
      "    accuracy                         0.5125       400\n",
      "   macro avg     0.6968    0.5135    0.4708       400\n",
      "weighted avg     0.6984    0.5125    0.4717       400\n",
      "\n",
      "22 0.0774263682681127 0.5175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6852    0.7255    0.7048       102\n",
      "         2.0     0.3621    0.8980    0.5161        98\n",
      "         3.0     0.8000    0.0800    0.1455       100\n",
      "\n",
      "    accuracy                         0.5175       400\n",
      "   macro avg     0.6990    0.5184    0.4747       400\n",
      "weighted avg     0.7006    0.5175    0.4756       400\n",
      "\n",
      "23 0.08497534359086446 0.5175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6852    0.7255    0.7048       102\n",
      "         2.0     0.3621    0.8980    0.5161        98\n",
      "         3.0     0.8000    0.0800    0.1455       100\n",
      "\n",
      "    accuracy                         0.5175       400\n",
      "   macro avg     0.6990    0.5184    0.4747       400\n",
      "weighted avg     0.7006    0.5175    0.4756       400\n",
      "\n",
      "24 0.093260334688322 0.5175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6852    0.7255    0.7048       102\n",
      "         2.0     0.3621    0.8980    0.5161        98\n",
      "         3.0     0.8000    0.0800    0.1455       100\n",
      "\n",
      "    accuracy                         0.5175       400\n",
      "   macro avg     0.6990    0.5184    0.4747       400\n",
      "weighted avg     0.7006    0.5175    0.4756       400\n",
      "\n",
      "25 0.10235310218990264 0.5225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6881    0.7353    0.7109       102\n",
      "         2.0     0.3651    0.8980    0.5192        98\n",
      "         3.0     0.8182    0.0900    0.1622       100\n",
      "\n",
      "    accuracy                         0.5225       400\n",
      "   macro avg     0.7050    0.5233    0.4812       400\n",
      "weighted avg     0.7066    0.5225    0.4821       400\n",
      "\n",
      "26 0.11233240329780277 0.5225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6818    0.7353    0.7075       102\n",
      "         2.0     0.3667    0.8980    0.5207        98\n",
      "         3.0     0.8182    0.0900    0.1622       100\n",
      "\n",
      "    accuracy                         0.5225       400\n",
      "   macro avg     0.7038    0.5233    0.4807       400\n",
      "weighted avg     0.7054    0.5225    0.4816       400\n",
      "\n",
      "27 0.12328467394420665 0.5225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6786    0.7451    0.7103       102\n",
      "         2.0     0.3671    0.8878    0.5194        98\n",
      "         3.0     0.7500    0.0900    0.1607       100\n",
      "\n",
      "    accuracy                         0.5225       400\n",
      "   macro avg     0.6861    0.5232    0.4807       400\n",
      "weighted avg     0.6877    0.5225    0.4816       400\n",
      "\n",
      "28 0.13530477745798075 0.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6786    0.7451    0.7103       102\n",
      "         2.0     0.3644    0.8776    0.5150        98\n",
      "         3.0     0.6923    0.0900    0.1593       100\n",
      "\n",
      "    accuracy                         0.5200       400\n",
      "   macro avg     0.6710    0.5207    0.4792       400\n",
      "weighted avg     0.6726    0.5200    0.4802       400\n",
      "\n",
      "29 0.1484968262254465 0.5175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6726    0.7451    0.7070       102\n",
      "         2.0     0.3617    0.8673    0.5105        98\n",
      "         3.0     0.6923    0.0900    0.1593       100\n",
      "\n",
      "    accuracy                         0.5175       400\n",
      "   macro avg     0.6688    0.5181    0.4773       400\n",
      "weighted avg     0.6704    0.5175    0.4783       400\n",
      "\n",
      "30 0.16297508346206444 0.5325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9487    0.3700    0.5324       100\n",
      "         1.0     0.6875    0.7549    0.7196       102\n",
      "         2.0     0.3668    0.8571    0.5138        98\n",
      "         3.0     0.7500    0.1500    0.2500       100\n",
      "\n",
      "    accuracy                         0.5325       400\n",
      "   macro avg     0.6883    0.5330    0.5039       400\n",
      "weighted avg     0.6899    0.5325    0.5050       400\n",
      "\n",
      "31 0.1788649529057435 0.5325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6875    0.7549    0.7196       102\n",
      "         2.0     0.3668    0.8571    0.5138        98\n",
      "         3.0     0.7619    0.1600    0.2645       100\n",
      "\n",
      "    accuracy                         0.5325       400\n",
      "   macro avg     0.6909    0.5330    0.5049       400\n",
      "weighted avg     0.6925    0.5325    0.5059       400\n",
      "\n",
      "32 0.19630406500402714 0.53\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6814    0.7549    0.7163       102\n",
      "         2.0     0.3640    0.8469    0.5092        98\n",
      "         3.0     0.7619    0.1600    0.2645       100\n",
      "\n",
      "    accuracy                         0.5300       400\n",
      "   macro avg     0.6887    0.5305    0.5029       400\n",
      "weighted avg     0.6903    0.5300    0.5040       400\n",
      "\n",
      "33 0.21544346900318845 0.5325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6814    0.7549    0.7163       102\n",
      "         2.0     0.3661    0.8367    0.5093        98\n",
      "         3.0     0.7200    0.1800    0.2880       100\n",
      "\n",
      "    accuracy                         0.5325       400\n",
      "   macro avg     0.6787    0.5329    0.5088       400\n",
      "weighted avg     0.6803    0.5325    0.5099       400\n",
      "\n",
      "34 0.23644894126454083 0.53\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6814    0.7549    0.7163       102\n",
      "         2.0     0.3620    0.8163    0.5016        98\n",
      "         3.0     0.6786    0.1900    0.2969       100\n",
      "\n",
      "    accuracy                         0.5300       400\n",
      "   macro avg     0.6673    0.5303    0.5091       400\n",
      "weighted avg     0.6689    0.5300    0.5102       400\n",
      "\n",
      "35 0.25950242113997374 0.535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6754    0.7549    0.7130       102\n",
      "         2.0     0.3657    0.8061    0.5032        98\n",
      "         3.0     0.6875    0.2200    0.3333       100\n",
      "\n",
      "    accuracy                         0.5350       400\n",
      "   macro avg     0.6690    0.5353    0.5178       400\n",
      "weighted avg     0.6706    0.5350    0.5189       400\n",
      "\n",
      "36 0.2848035868435802 0.5325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6754    0.7549    0.7130       102\n",
      "         2.0     0.3615    0.7857    0.4952        98\n",
      "         3.0     0.6571    0.2300    0.3407       100\n",
      "\n",
      "    accuracy                         0.5325       400\n",
      "   macro avg     0.6604    0.5327    0.5177       400\n",
      "weighted avg     0.6619    0.5325    0.5187       400\n",
      "\n",
      "37 0.3125715849688237 0.5325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6724    0.7647    0.7156       102\n",
      "         2.0     0.3619    0.7755    0.4935        98\n",
      "         3.0     0.6389    0.2300    0.3382       100\n",
      "\n",
      "    accuracy                         0.5325       400\n",
      "   macro avg     0.6551    0.5326    0.5173       400\n",
      "weighted avg     0.6567    0.5325    0.5184       400\n",
      "\n",
      "38 0.34304692863149194 0.545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6780    0.7843    0.7273       102\n",
      "         2.0     0.3668    0.7449    0.4916        98\n",
      "         3.0     0.6444    0.2900    0.4000       100\n",
      "\n",
      "    accuracy                         0.5450       400\n",
      "   macro avg     0.6592    0.5448    0.5351       400\n",
      "weighted avg     0.6607    0.5450    0.5363       400\n",
      "\n",
      "39 0.37649358067924693 0.5475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6780    0.7843    0.7273       102\n",
      "         2.0     0.3687    0.7449    0.4932        98\n",
      "         3.0     0.6522    0.3000    0.4110       100\n",
      "\n",
      "    accuracy                         0.5475       400\n",
      "   macro avg     0.6615    0.5473    0.5383       400\n",
      "weighted avg     0.6631    0.5475    0.5395       400\n",
      "\n",
      "40 0.41320124001153385 0.545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6780    0.7843    0.7273       102\n",
      "         2.0     0.3660    0.7245    0.4863        98\n",
      "         3.0     0.6200    0.3100    0.4133       100\n",
      "\n",
      "    accuracy                         0.5450       400\n",
      "   macro avg     0.6528    0.5447    0.5372       400\n",
      "weighted avg     0.6544    0.5450    0.5384       400\n",
      "\n",
      "41 0.4534878508128584 0.55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9474    0.3600    0.5217       100\n",
      "         1.0     0.6723    0.7843    0.7240       102\n",
      "         2.0     0.3670    0.7041    0.4825        98\n",
      "         3.0     0.6364    0.3500    0.4516       100\n",
      "\n",
      "    accuracy                         0.5500       400\n",
      "   macro avg     0.6558    0.5496    0.5450       400\n",
      "weighted avg     0.6573    0.5500    0.5462       400\n",
      "\n",
      "42 0.49770235643321115 0.5575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9500    0.3800    0.5429       100\n",
      "         1.0     0.6723    0.7843    0.7240       102\n",
      "         2.0     0.3750    0.7041    0.4894        98\n",
      "         3.0     0.6316    0.3600    0.4586       100\n",
      "\n",
      "    accuracy                         0.5575       400\n",
      "   macro avg     0.6572    0.5571    0.5537       400\n",
      "weighted avg     0.6587    0.5575    0.5549       400\n",
      "\n",
      "43 0.5462277217684343 0.575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9500    0.3800    0.5429       100\n",
      "         1.0     0.6667    0.7843    0.7207       102\n",
      "         2.0     0.3920    0.7041    0.5036        98\n",
      "         3.0     0.6719    0.4300    0.5244       100\n",
      "\n",
      "    accuracy                         0.5750       400\n",
      "   macro avg     0.6701    0.5746    0.5729       400\n",
      "weighted avg     0.6715    0.5750    0.5740       400\n",
      "\n",
      "44 0.5994842503189411 0.5775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9500    0.3800    0.5429       100\n",
      "         1.0     0.6694    0.7941    0.7265       102\n",
      "         2.0     0.3882    0.6735    0.4925        98\n",
      "         3.0     0.6667    0.4600    0.5444       100\n",
      "\n",
      "    accuracy                         0.5775       400\n",
      "   macro avg     0.6686    0.5769    0.5766       400\n",
      "weighted avg     0.6700    0.5775    0.5777       400\n",
      "\n",
      "45 0.6579332246575682 0.585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9512    0.3900    0.5532       100\n",
      "         1.0     0.6750    0.7941    0.7297       102\n",
      "         2.0     0.3916    0.6633    0.4924        98\n",
      "         3.0     0.6712    0.4900    0.5665       100\n",
      "\n",
      "    accuracy                         0.5850       400\n",
      "   macro avg     0.6723    0.5843    0.5855       400\n",
      "weighted avg     0.6737    0.5850    0.5866       400\n",
      "\n",
      "46 0.7220809018385468 0.585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9512    0.3900    0.5532       100\n",
      "         1.0     0.6750    0.7941    0.7297       102\n",
      "         2.0     0.3916    0.6633    0.4924        98\n",
      "         3.0     0.6712    0.4900    0.5665       100\n",
      "\n",
      "    accuracy                         0.5850       400\n",
      "   macro avg     0.6723    0.5843    0.5855       400\n",
      "weighted avg     0.6737    0.5850    0.5866       400\n",
      "\n",
      "47 0.7924828983539177 0.5925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9524    0.4000    0.5634       100\n",
      "         1.0     0.6750    0.7941    0.7297       102\n",
      "         2.0     0.3988    0.6633    0.4981        98\n",
      "         3.0     0.6800    0.5100    0.5829       100\n",
      "\n",
      "    accuracy                         0.5925       400\n",
      "   macro avg     0.6765    0.5918    0.5935       400\n",
      "weighted avg     0.6779    0.5925    0.5947       400\n",
      "\n",
      "48 0.8697490026177834 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9149    0.4300    0.5850       100\n",
      "         1.0     0.6750    0.7941    0.7297       102\n",
      "         2.0     0.4076    0.6531    0.5020        98\n",
      "         3.0     0.6842    0.5200    0.5909       100\n",
      "\n",
      "    accuracy                         0.6000       400\n",
      "   macro avg     0.6704    0.5993    0.6019       400\n",
      "weighted avg     0.6718    0.6000    0.6030       400\n",
      "\n",
      "49 0.9545484566618342 0.5975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8776    0.4300    0.5772       100\n",
      "         1.0     0.6807    0.7941    0.7330       102\n",
      "         2.0     0.4040    0.6224    0.4900        98\n",
      "         3.0     0.6667    0.5400    0.5967       100\n",
      "\n",
      "    accuracy                         0.5975       400\n",
      "   macro avg     0.6572    0.5966    0.5992       400\n",
      "weighted avg     0.6586    0.5975    0.6004       400\n",
      "\n",
      "50 1.0476157527896652 0.6075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8519    0.4600    0.5974       100\n",
      "         1.0     0.6864    0.7941    0.7364       102\n",
      "         2.0     0.4126    0.6020    0.4896        98\n",
      "         3.0     0.6706    0.5700    0.6162       100\n",
      "\n",
      "    accuracy                         0.6075       400\n",
      "   macro avg     0.6554    0.6065    0.6099       400\n",
      "weighted avg     0.6567    0.6075    0.6111       400\n",
      "\n",
      "51 1.149756995397737 0.6175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8500    0.5100    0.6375       100\n",
      "         1.0     0.6991    0.7745    0.7349       102\n",
      "         2.0     0.4255    0.6122    0.5021        98\n",
      "         3.0     0.6628    0.5700    0.6129       100\n",
      "\n",
      "    accuracy                         0.6175       400\n",
      "   macro avg     0.6594    0.6167    0.6218       400\n",
      "weighted avg     0.6607    0.6175    0.6230       400\n",
      "\n",
      "52 1.261856883066021 0.6125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8154    0.5300    0.6424       100\n",
      "         1.0     0.7091    0.7647    0.7358       102\n",
      "         2.0     0.4234    0.5918    0.4936        98\n",
      "         3.0     0.6364    0.5600    0.5957       100\n",
      "\n",
      "    accuracy                         0.6125       400\n",
      "   macro avg     0.6460    0.6116    0.6169       400\n",
      "weighted avg     0.6475    0.6125    0.6181       400\n",
      "\n",
      "53 1.384886371393873 0.6225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8000    0.5600    0.6588       100\n",
      "         1.0     0.7264    0.7549    0.7404       102\n",
      "         2.0     0.4370    0.6020    0.5064        98\n",
      "         3.0     0.6404    0.5700    0.6032       100\n",
      "\n",
      "    accuracy                         0.6225       400\n",
      "   macro avg     0.6510    0.6217    0.6272       400\n",
      "weighted avg     0.6524    0.6225    0.6284       400\n",
      "\n",
      "54 1.5199110829529348 0.6275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7692    0.6000    0.6742       100\n",
      "         1.0     0.7451    0.7451    0.7451       102\n",
      "         2.0     0.4394    0.5918    0.5043        98\n",
      "         3.0     0.6477    0.5700    0.6064       100\n",
      "\n",
      "    accuracy                         0.6275       400\n",
      "   macro avg     0.6504    0.6267    0.6325       400\n",
      "weighted avg     0.6519    0.6275    0.6337       400\n",
      "\n",
      "55 1.6681005372000592 0.6275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7191    0.6400    0.6772       100\n",
      "         1.0     0.7604    0.7157    0.7374       102\n",
      "         2.0     0.4444    0.5714    0.5000        98\n",
      "         3.0     0.6517    0.5800    0.6138       100\n",
      "\n",
      "    accuracy                         0.6275       400\n",
      "   macro avg     0.6439    0.6268    0.6321       400\n",
      "weighted avg     0.6455    0.6275    0.6333       400\n",
      "\n",
      "56 1.8307382802953698 0.6225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7111    0.6400    0.6737       100\n",
      "         1.0     0.7604    0.7157    0.7374       102\n",
      "         2.0     0.4355    0.5510    0.4865        98\n",
      "         3.0     0.6444    0.5800    0.6105       100\n",
      "\n",
      "    accuracy                         0.6225       400\n",
      "   macro avg     0.6379    0.6217    0.6270       400\n",
      "weighted avg     0.6395    0.6225    0.6283       400\n",
      "\n",
      "57 2.0092330025650478 0.6175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6947    0.6600    0.6769       100\n",
      "         1.0     0.7717    0.6961    0.7320       102\n",
      "         2.0     0.4298    0.5306    0.4749        98\n",
      "         3.0     0.6304    0.5800    0.6042       100\n",
      "\n",
      "    accuracy                         0.6175       400\n",
      "   macro avg     0.6317    0.6167    0.6220       400\n",
      "weighted avg     0.6334    0.6175    0.6233       400\n",
      "\n",
      "58 2.2051307399030455 0.6175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7010    0.6800    0.6904       100\n",
      "         1.0     0.7640    0.6667    0.7120       102\n",
      "         2.0     0.4322    0.5204    0.4722        98\n",
      "         3.0     0.6250    0.6000    0.6122       100\n",
      "\n",
      "    accuracy                         0.6175       400\n",
      "   macro avg     0.6306    0.6168    0.6217       400\n",
      "weighted avg     0.6322    0.6175    0.6229       400\n",
      "\n",
      "59 2.4201282647943834 0.625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7000    0.7000    0.7000       100\n",
      "         1.0     0.7791    0.6569    0.7128       102\n",
      "         2.0     0.4444    0.5306    0.4837        98\n",
      "         3.0     0.6289    0.6100    0.6193       100\n",
      "\n",
      "    accuracy                         0.6250       400\n",
      "   macro avg     0.6381    0.6244    0.6289       400\n",
      "weighted avg     0.6398    0.6250    0.6301       400\n",
      "\n",
      "60 2.656087782946687 0.6275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6857    0.7200    0.7024       100\n",
      "         1.0     0.7738    0.6373    0.6989       102\n",
      "         2.0     0.4602    0.5306    0.4929        98\n",
      "         3.0     0.6327    0.6200    0.6263       100\n",
      "\n",
      "    accuracy                         0.6275       400\n",
      "   macro avg     0.6381    0.6270    0.6301       400\n",
      "weighted avg     0.6397    0.6275    0.6312       400\n",
      "\n",
      "61 2.9150530628251787 0.625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6636    0.7300    0.6952       100\n",
      "         1.0     0.7778    0.6176    0.6885       102\n",
      "         2.0     0.4602    0.5306    0.4929        98\n",
      "         3.0     0.6458    0.6200    0.6327       100\n",
      "\n",
      "    accuracy                         0.6250       400\n",
      "   macro avg     0.6369    0.6246    0.6273       400\n",
      "weighted avg     0.6384    0.6250    0.6283       400\n",
      "\n",
      "62 3.1992671377973845 0.615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6549    0.7400    0.6948       100\n",
      "         1.0     0.7625    0.5980    0.6703       102\n",
      "         2.0     0.4404    0.4898    0.4638        98\n",
      "         3.0     0.6429    0.6300    0.6364       100\n",
      "\n",
      "    accuracy                         0.6150       400\n",
      "   macro avg     0.6251    0.6145    0.6163       400\n",
      "weighted avg     0.6268    0.6150    0.6174       400\n",
      "\n",
      "63 3.5111917342151346 0.61\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6410    0.7500    0.6912       100\n",
      "         1.0     0.7532    0.5686    0.6480       102\n",
      "         2.0     0.4444    0.4898    0.4660        98\n",
      "         3.0     0.6429    0.6300    0.6364       100\n",
      "\n",
      "    accuracy                         0.6100       400\n",
      "   macro avg     0.6204    0.6096    0.6104       400\n",
      "weighted avg     0.6219    0.6100    0.6113       400\n",
      "\n",
      "64 3.8535285937105312 0.6175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6356    0.7500    0.6881       100\n",
      "         1.0     0.7500    0.5588    0.6404       102\n",
      "         2.0     0.4679    0.5204    0.4928        98\n",
      "         3.0     0.6598    0.6400    0.6497       100\n",
      "\n",
      "    accuracy                         0.6175       400\n",
      "   macro avg     0.6283    0.6173    0.6178       400\n",
      "weighted avg     0.6297    0.6175    0.6185       400\n",
      "\n",
      "65 4.229242874389499 0.6125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6333    0.7600    0.6909       100\n",
      "         1.0     0.7403    0.5588    0.6369       102\n",
      "         2.0     0.4630    0.5102    0.4854        98\n",
      "         3.0     0.6526    0.6200    0.6359       100\n",
      "\n",
      "    accuracy                         0.6125       400\n",
      "   macro avg     0.6223    0.6123    0.6123       400\n",
      "weighted avg     0.6237    0.6125    0.6130       400\n",
      "\n",
      "66 4.641588833612782 0.6175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6417    0.7700    0.7000       100\n",
      "         1.0     0.7368    0.5490    0.6292       102\n",
      "         2.0     0.4771    0.5306    0.5024        98\n",
      "         3.0     0.6526    0.6200    0.6359       100\n",
      "\n",
      "    accuracy                         0.6175       400\n",
      "   macro avg     0.6271    0.6174    0.6169       400\n",
      "weighted avg     0.6284    0.6175    0.6175       400\n",
      "\n",
      "67 5.09413801481638 0.6275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6583    0.7900    0.7182       100\n",
      "         1.0     0.7403    0.5588    0.6369       102\n",
      "         2.0     0.4818    0.5408    0.5096        98\n",
      "         3.0     0.6667    0.6200    0.6425       100\n",
      "\n",
      "    accuracy                         0.6275       400\n",
      "   macro avg     0.6368    0.6274    0.6268       400\n",
      "weighted avg     0.6381    0.6275    0.6274       400\n",
      "\n",
      "68 5.590810182512229 0.6225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6583    0.7900    0.7182       100\n",
      "         1.0     0.7308    0.5588    0.6333       102\n",
      "         2.0     0.4722    0.5204    0.4951        98\n",
      "         3.0     0.6596    0.6200    0.6392       100\n",
      "\n",
      "    accuracy                         0.6225       400\n",
      "   macro avg     0.6302    0.6223    0.6215       400\n",
      "weighted avg     0.6315    0.6225    0.6221       400\n",
      "\n",
      "69 6.135907273413176 0.615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6557    0.8000    0.7207       100\n",
      "         1.0     0.7368    0.5490    0.6292       102\n",
      "         2.0     0.4571    0.4898    0.4729        98\n",
      "         3.0     0.6392    0.6200    0.6294       100\n",
      "\n",
      "    accuracy                         0.6150       400\n",
      "   macro avg     0.6222    0.6147    0.6131       400\n",
      "weighted avg     0.6236    0.6150    0.6139       400\n",
      "\n",
      "70 6.7341506577508286 0.6075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6504    0.8000    0.7175       100\n",
      "         1.0     0.7179    0.5490    0.6222       102\n",
      "         2.0     0.4455    0.4592    0.4523        98\n",
      "         3.0     0.6327    0.6200    0.6263       100\n",
      "\n",
      "    accuracy                         0.6075       400\n",
      "   macro avg     0.6116    0.6071    0.6046       400\n",
      "weighted avg     0.6130    0.6075    0.6054       400\n",
      "\n",
      "71 7.390722033525782 0.6025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6371    0.7900    0.7054       100\n",
      "         1.0     0.7215    0.5588    0.6298       102\n",
      "         2.0     0.4343    0.4388    0.4365        98\n",
      "         3.0     0.6327    0.6200    0.6263       100\n",
      "\n",
      "    accuracy                         0.6025       400\n",
      "   macro avg     0.6064    0.6019    0.5995       400\n",
      "weighted avg     0.6078    0.6025    0.6005       400\n",
      "\n",
      "72 8.111308307896872 0.6125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6320    0.7900    0.7022       100\n",
      "         1.0     0.7308    0.5588    0.6333       102\n",
      "         2.0     0.4545    0.4592    0.4569        98\n",
      "         3.0     0.6531    0.6400    0.6465       100\n",
      "\n",
      "    accuracy                         0.6125       400\n",
      "   macro avg     0.6176    0.6120    0.6097       400\n",
      "weighted avg     0.6190    0.6125    0.6106       400\n",
      "\n",
      "73 8.902150854450392 0.615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6299    0.8000    0.7048       100\n",
      "         1.0     0.7308    0.5588    0.6333       102\n",
      "         2.0     0.4639    0.4592    0.4615        98\n",
      "         3.0     0.6531    0.6400    0.6465       100\n",
      "\n",
      "    accuracy                         0.6150       400\n",
      "   macro avg     0.6194    0.6145    0.6115       400\n",
      "weighted avg     0.6208    0.6150    0.6124       400\n",
      "\n",
      "74 9.770099572992256 0.615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6423    0.7900    0.7085       100\n",
      "         1.0     0.7215    0.5588    0.6298       102\n",
      "         2.0     0.4639    0.4592    0.4615        98\n",
      "         3.0     0.6436    0.6500    0.6468       100\n",
      "\n",
      "    accuracy                         0.6150       400\n",
      "   macro avg     0.6178    0.6145    0.6117       400\n",
      "weighted avg     0.6191    0.6150    0.6125       400\n",
      "\n",
      "75 10.722672220103243 0.6075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6220    0.7900    0.6960       100\n",
      "         1.0     0.7105    0.5294    0.6067       102\n",
      "         2.0     0.4688    0.4592    0.4639        98\n",
      "         3.0     0.6436    0.6500    0.6468       100\n",
      "\n",
      "    accuracy                         0.6075       400\n",
      "   macro avg     0.6112    0.6071    0.6034       400\n",
      "weighted avg     0.6124    0.6075    0.6041       400\n",
      "\n",
      "76 11.768119524349991 0.605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6190    0.7800    0.6903       100\n",
      "         1.0     0.7105    0.5294    0.6067       102\n",
      "         2.0     0.4639    0.4592    0.4615        98\n",
      "         3.0     0.6436    0.6500    0.6468       100\n",
      "\n",
      "    accuracy                         0.6050       400\n",
      "   macro avg     0.6093    0.6046    0.6013       400\n",
      "weighted avg     0.6105    0.6050    0.6021       400\n",
      "\n",
      "77 12.91549665014884 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6172    0.7900    0.6930       100\n",
      "         1.0     0.7027    0.5098    0.5909       102\n",
      "         2.0     0.4545    0.4592    0.4569        98\n",
      "         3.0     0.6465    0.6400    0.6432       100\n",
      "\n",
      "    accuracy                         0.6000       400\n",
      "   macro avg     0.6052    0.5997    0.5960       400\n",
      "weighted avg     0.6065    0.6000    0.5967       400\n",
      "\n",
      "78 14.174741629268063 0.605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6202    0.8000    0.6987       100\n",
      "         1.0     0.7162    0.5196    0.6023       102\n",
      "         2.0     0.4592    0.4592    0.4592        98\n",
      "         3.0     0.6465    0.6400    0.6432       100\n",
      "\n",
      "    accuracy                         0.6050       400\n",
      "   macro avg     0.6105    0.6047    0.6008       400\n",
      "weighted avg     0.6118    0.6050    0.6016       400\n",
      "\n",
      "79 15.556761439304722 0.5925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6016    0.7700    0.6754       100\n",
      "         1.0     0.6933    0.5098    0.5876       102\n",
      "         2.0     0.4490    0.4490    0.4490        98\n",
      "         3.0     0.6465    0.6400    0.6432       100\n",
      "\n",
      "    accuracy                         0.5925       400\n",
      "   macro avg     0.5976    0.5922    0.5888       400\n",
      "weighted avg     0.5988    0.5925    0.5895       400\n",
      "\n",
      "80 17.07352647470692 0.5925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6080    0.7600    0.6756       100\n",
      "         1.0     0.6883    0.5196    0.5922       102\n",
      "         2.0     0.4444    0.4490    0.4467        98\n",
      "         3.0     0.6465    0.6400    0.6432       100\n",
      "\n",
      "    accuracy                         0.5925       400\n",
      "   macro avg     0.5968    0.5921    0.5894       400\n",
      "weighted avg     0.5980    0.5925    0.5901       400\n",
      "\n",
      "81 18.73817422860385 0.595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6080    0.7600    0.6756       100\n",
      "         1.0     0.6750    0.5294    0.5934       102\n",
      "         2.0     0.4583    0.4490    0.4536        98\n",
      "         3.0     0.6465    0.6400    0.6432       100\n",
      "\n",
      "    accuracy                         0.5950       400\n",
      "   macro avg     0.5969    0.5946    0.5914       400\n",
      "weighted avg     0.5980    0.5950    0.5921       400\n",
      "\n",
      "82 20.565123083486537 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6179    0.7600    0.6816       100\n",
      "         1.0     0.6750    0.5294    0.5934       102\n",
      "         2.0     0.4694    0.4694    0.4694        98\n",
      "         3.0     0.6465    0.6400    0.6432       100\n",
      "\n",
      "    accuracy                         0.6000       400\n",
      "   macro avg     0.6022    0.5997    0.5969       400\n",
      "weighted avg     0.6032    0.6000    0.5975       400\n",
      "\n",
      "83 22.570197196339215 0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6179    0.7600    0.6816       100\n",
      "         1.0     0.6750    0.5294    0.5934       102\n",
      "         2.0     0.4694    0.4694    0.4694        98\n",
      "         3.0     0.6465    0.6400    0.6432       100\n",
      "\n",
      "    accuracy                         0.6000       400\n",
      "   macro avg     0.6022    0.5997    0.5969       400\n",
      "weighted avg     0.6032    0.6000    0.5975       400\n",
      "\n",
      "84 24.770763559917114 0.5975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6148    0.7500    0.6757       100\n",
      "         1.0     0.6667    0.5294    0.5902       102\n",
      "         2.0     0.4694    0.4694    0.4694        98\n",
      "         3.0     0.6465    0.6400    0.6432       100\n",
      "\n",
      "    accuracy                         0.5975       400\n",
      "   macro avg     0.5993    0.5972    0.5946       400\n",
      "weighted avg     0.6003    0.5975    0.5952       400\n",
      "\n",
      "85 27.185882427329428 0.5925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6066    0.7400    0.6667       100\n",
      "         1.0     0.6667    0.5294    0.5902       102\n",
      "         2.0     0.4639    0.4592    0.4615        98\n",
      "         3.0     0.6400    0.6400    0.6400       100\n",
      "\n",
      "    accuracy                         0.5925       400\n",
      "   macro avg     0.5943    0.5921    0.5896       400\n",
      "weighted avg     0.5953    0.5925    0.5902       400\n",
      "\n",
      "86 29.836472402833405 0.5975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6148    0.7500    0.6757       100\n",
      "         1.0     0.6790    0.5392    0.6011       102\n",
      "         2.0     0.4646    0.4694    0.4670        98\n",
      "         3.0     0.6429    0.6300    0.6364       100\n",
      "\n",
      "    accuracy                         0.5975       400\n",
      "   macro avg     0.6003    0.5972    0.5950       400\n",
      "weighted avg     0.6014    0.5975    0.5957       400\n",
      "\n",
      "87 32.745491628777316 0.595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6098    0.7500    0.6726       100\n",
      "         1.0     0.6790    0.5392    0.6011       102\n",
      "         2.0     0.4646    0.4694    0.4670        98\n",
      "         3.0     0.6392    0.6200    0.6294       100\n",
      "\n",
      "    accuracy                         0.5950       400\n",
      "   macro avg     0.5981    0.5947    0.5925       400\n",
      "weighted avg     0.5992    0.5950    0.5932       400\n",
      "\n",
      "88 35.938136638046295 0.595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6179    0.7600    0.6816       100\n",
      "         1.0     0.6835    0.5294    0.5967       102\n",
      "         2.0     0.4554    0.4694    0.4623        98\n",
      "         3.0     0.6392    0.6200    0.6294       100\n",
      "\n",
      "    accuracy                         0.5950       400\n",
      "   macro avg     0.5990    0.5947    0.5925       400\n",
      "weighted avg     0.6002    0.5950    0.5932       400\n",
      "\n",
      "89 39.4420605943766 0.5925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5984    0.7600    0.6696       100\n",
      "         1.0     0.6842    0.5098    0.5843       102\n",
      "         2.0     0.4653    0.4796    0.4724        98\n",
      "         3.0     0.6458    0.6200    0.6327       100\n",
      "\n",
      "    accuracy                         0.5925       400\n",
      "   macro avg     0.5985    0.5923    0.5897       400\n",
      "weighted avg     0.5995    0.5925    0.5903       400\n",
      "\n",
      "90 43.287612810830616 0.5875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5846    0.7600    0.6609       100\n",
      "         1.0     0.6800    0.5000    0.5763       102\n",
      "         2.0     0.4700    0.4796    0.4747        98\n",
      "         3.0     0.6421    0.6100    0.6256       100\n",
      "\n",
      "    accuracy                         0.5875       400\n",
      "   macro avg     0.5942    0.5874    0.5844       400\n",
      "weighted avg     0.5952    0.5875    0.5849       400\n",
      "\n",
      "91 47.50810162102798 0.585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5630    0.7600    0.6468       100\n",
      "         1.0     0.6933    0.5098    0.5876       102\n",
      "         2.0     0.4688    0.4592    0.4639        98\n",
      "         3.0     0.6489    0.6100    0.6289       100\n",
      "\n",
      "    accuracy                         0.5850       400\n",
      "   macro avg     0.5935    0.5847    0.5818       400\n",
      "weighted avg     0.5946    0.5850    0.5824       400\n",
      "\n",
      "92 52.140082879996896 0.58\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5547    0.7600    0.6414       100\n",
      "         1.0     0.6892    0.5000    0.5795       102\n",
      "         2.0     0.4688    0.4592    0.4639        98\n",
      "         3.0     0.6452    0.6000    0.6218       100\n",
      "\n",
      "    accuracy                         0.5800       400\n",
      "   macro avg     0.5895    0.5798    0.5766       400\n",
      "weighted avg     0.5906    0.5800    0.5772       400\n",
      "\n",
      "93 57.223676593502205 0.5775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5429    0.7600    0.6333       100\n",
      "         1.0     0.6849    0.4902    0.5714       102\n",
      "         2.0     0.4737    0.4592    0.4663        98\n",
      "         3.0     0.6522    0.6000    0.6250       100\n",
      "\n",
      "    accuracy                         0.5775       400\n",
      "   macro avg     0.5884    0.5773    0.5740       400\n",
      "weighted avg     0.5895    0.5775    0.5745       400\n",
      "\n",
      "94 62.802914418342596 0.5625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5205    0.7600    0.6179       100\n",
      "         1.0     0.6714    0.4608    0.5465       102\n",
      "         2.0     0.4731    0.4490    0.4607        98\n",
      "         3.0     0.6374    0.5800    0.6073       100\n",
      "\n",
      "    accuracy                         0.5625       400\n",
      "   macro avg     0.5756    0.5624    0.5581       400\n",
      "weighted avg     0.5766    0.5625    0.5585       400\n",
      "\n",
      "95 68.92612104349702 0.5475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5067    0.7600    0.6080       100\n",
      "         1.0     0.6567    0.4314    0.5207       102\n",
      "         2.0     0.4574    0.4388    0.4479        98\n",
      "         3.0     0.6292    0.5600    0.5926       100\n",
      "\n",
      "    accuracy                         0.5475       400\n",
      "   macro avg     0.5625    0.5475    0.5423       400\n",
      "weighted avg     0.5635    0.5475    0.5427       400\n",
      "\n",
      "96 75.64633275546291 0.5425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4841    0.7600    0.5914       100\n",
      "         1.0     0.6515    0.4216    0.5119       102\n",
      "         2.0     0.4719    0.4286    0.4492        98\n",
      "         3.0     0.6364    0.5600    0.5957       100\n",
      "\n",
      "    accuracy                         0.5425       400\n",
      "   macro avg     0.5610    0.5425    0.5371       400\n",
      "weighted avg     0.5619    0.5425    0.5374       400\n",
      "\n",
      "97 83.02175681319753 0.5275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4578    0.7600    0.5714       100\n",
      "         1.0     0.6508    0.4020    0.4970       102\n",
      "         2.0     0.4828    0.4286    0.4541        98\n",
      "         3.0     0.6190    0.5200    0.5652       100\n",
      "\n",
      "    accuracy                         0.5275       400\n",
      "   macro avg     0.5526    0.5276    0.5219       400\n",
      "weighted avg     0.5534    0.5275    0.5221       400\n",
      "\n",
      "98 91.11627561154896 0.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4318    0.7600    0.5507       100\n",
      "         1.0     0.6610    0.3824    0.4845       102\n",
      "         2.0     0.4699    0.3980    0.4309        98\n",
      "         3.0     0.6098    0.5000    0.5495       100\n",
      "\n",
      "    accuracy                         0.5100       400\n",
      "   macro avg     0.5431    0.5101    0.5039       400\n",
      "weighted avg     0.5441    0.5100    0.5042       400\n",
      "\n",
      "99 100.0 0.495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.4074    0.7700    0.5329       100\n",
      "         1.0     0.6667    0.3725    0.4780       102\n",
      "         2.0     0.4675    0.3673    0.4114        98\n",
      "         3.0     0.6104    0.4700    0.5311       100\n",
      "\n",
      "    accuracy                         0.4950       400\n",
      "   macro avg     0.5380    0.4950    0.4883       400\n",
      "weighted avg     0.5390    0.4950    0.4887       400\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6446    0.7800    0.7059       100\n",
      "         1.0     0.7368    0.5490    0.6292       102\n",
      "         2.0     0.4862    0.5408    0.5121        98\n",
      "         3.0     0.6596    0.6200    0.6392       100\n",
      "\n",
      "    accuracy                         0.6225       400\n",
      "   macro avg     0.6318    0.6225    0.6216       400\n",
      "weighted avg     0.6331    0.6225    0.6222       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 后端网络LPA分类\n",
    "import numpy as np\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X = features  # 输入\n",
    "y = label  # 输出\n",
    "rng = np.random.RandomState(9)  # 随机数种子\n",
    "indices = np.arange(len(y))\n",
    "rng.shuffle(indices)  # 将索引打乱\n",
    "# print(X[0])\n",
    "\n",
    "n_total_samples = len(y)\n",
    "# n_labeled_points = 2795  # 已标记/样本数\n",
    "n_labeled_points = 1600  # 已标记样本数1260 1440\n",
    "\n",
    "unlabeled_set = indices[n_labeled_points:]\n",
    "\n",
    "y_train = np.copy(y)\n",
    "y_train[unlabeled_set] = -1  # 未标记样本集y设为-1\n",
    "\n",
    "num=100\n",
    "accuracy = []\n",
    "gammas = np.logspace(-2, 2, num=num)\n",
    "# gammas = range(100)\n",
    "score_in = 0\n",
    "for i in range(num):\n",
    "    lp_model = LabelSpreading(gamma=gammas[i], max_iter=30, kernel='rbf')\n",
    "    lp_model.fit(X, y_train)\n",
    "    predicted_labels = lp_model.transduction_[unlabeled_set]\n",
    "    true_labels = y[unlabeled_set]\n",
    "    score = accuracy_score(true_labels, predicted_labels)\n",
    "    # if score > score_in:\n",
    "    #     gamma_fin = gammas[i]\n",
    "    accuracy.append(score)\n",
    "    print(i,gammas[i],score)\n",
    "    print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "\n",
    "# lp_model = LabelSpreading(gamma=0.24, max_iter=200, kernel='rbf')# bert2000\n",
    "lp_model = LabelSpreading(gamma=4.75, max_iter=200, kernel='rbf')# tf-idf2000\n",
    "# lp_model = LabelSpreading(kernel='knn', n_neighbors=9, max_iter=100)\n",
    "lp_model.fit(X, y_train)\n",
    "predicted_labels = lp_model.transduction_[unlabeled_set]\n",
    "true_labels = y[unlabeled_set]\n",
    "print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "\n",
    "\n",
    "\n",
    "# print(\n",
    "#     \"Label Spreading model: %d labeled & %d unlabeled points (%d total)\"\n",
    "#     % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples)\n",
    "# )\n",
    "# print(classification_report(true_labels, predicted_labels))\n",
    "# print(true_labels)\n",
    "# print(predicted_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6348    0.7374    0.6822        99\n",
      "         1.0     0.7576    0.6637    0.7075       113\n",
      "         2.0     0.5146    0.5300    0.5222       100\n",
      "         3.0     0.6506    0.6136    0.6316        88\n",
      "\n",
      "    accuracy                         0.6375       400\n",
      "   macro avg     0.6394    0.6362    0.6359       400\n",
      "weighted avg     0.6429    0.6375    0.6382       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lp_model = LabelSpreading(gamma=7.4, max_iter=200, kernel='rbf')# tf-idf2000\n",
    "# lp_model = LabelSpreading(kernel='knn', n_neighbors=9, max_iter=100)\n",
    "lp_model.fit(X, y_train)\n",
    "predicted_labels = lp_model.transduction_[unlabeled_set]\n",
    "true_labels = y[unlabeled_set]\n",
    "print(classification_report(true_labels, predicted_labels, digits=4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6486    0.7200    0.6825       100\n",
      "         1.0     0.7412    0.6176    0.6738       102\n",
      "         2.0     0.4318    0.3878    0.4086        98\n",
      "         3.0     0.5862    0.6800    0.6296       100\n",
      "\n",
      "    accuracy                         0.6025       400\n",
      "   macro avg     0.6020    0.6014    0.5986       400\n",
      "weighted avg     0.6035    0.6025    0.5999       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = features  # 输入\n",
    "y = label  # 输出\n",
    "\n",
    "rng = np.random.RandomState(9)  # 随机数种子\n",
    "indices = np.arange(len(y))\n",
    "rng.shuffle(indices)  # 将索引打乱\n",
    "# print(X[0])\n",
    "\n",
    "n_total_samples = len(y)\n",
    "n_labeled_points = 1600  # 已标记样本数\n",
    "\n",
    "unlabeled_set = indices[n_labeled_points:]\n",
    "labeled_set = indices[:n_labeled_points]\n",
    "X_train = X[labeled_set]\n",
    "y_train = y[labeled_set]\n",
    "X_test = X[unlabeled_set]\n",
    "y_true = y[unlabeled_set]\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=20)\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "print(classification_report(y_true, y_predict, digits=4))\n",
    "\n",
    "# # 设置弱学习器数量为10\n",
    "# for i in range(1,100):\n",
    "#     model = RandomForestClassifier(n_estimators=i)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_predict = model.predict(X_test)\n",
    "#     print(i, accuracy_score(y_true, y_predict))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6854    0.6162    0.6489        99\n",
      "         1.0     0.6522    0.7965    0.7171       113\n",
      "         2.0     0.5000    0.4600    0.4792       100\n",
      "         3.0     0.6543    0.6023    0.6272        88\n",
      "\n",
      "    accuracy                         0.6250       400\n",
      "   macro avg     0.6230    0.6187    0.6181       400\n",
      "weighted avg     0.6228    0.6250    0.6210       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=39)\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "print(classification_report(y_true, y_predict, digits=4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}