{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "参考代码：https://blog.csdn.net/weixin_42608414/article/details/88391760\n",
    "'''\n",
    "import re\n",
    "#定义删除除字母,数字，汉字以外的所有符号的函数\n",
    "def remove_punctuation(line):\n",
    "    line = str(line)\n",
    "    if line.strip()=='':\n",
    "        return ''\n",
    "    rule = re.compile(u\"[^a-zA-Z0-9\\u4E00-\\u9FA5]\")\n",
    "    line = rule.sub('',line)\n",
    "    return line\n",
    "\n",
    "#停用词列表\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "#加载停用词\n",
    "stopwords = stopwordslist(\"./stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据总量: 2000 .\n"
     ]
    },
    {
     "data": {
      "text/plain": "                      text  label\n1986  亲爱的老师，解释一下为什么是这样子的谢谢      3\n1837            这题怎么做呀，谢谢啦      3\n634               表示这道题不理解      1\n927              刚刚在网课上看到的      1\n693           不过你是怎么倒过来了一下      1\n571           直接洛必达一下就解决了啊      1\n1880    向老师求助请老师帮忙解答一下这三道题      3\n1631                咋做呢，老师      3\n984              因为是任意给定的E      1\n1289         老师，这些是为什么啊求解，      2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1986</th>\n      <td>亲爱的老师，解释一下为什么是这样子的谢谢</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1837</th>\n      <td>这题怎么做呀，谢谢啦</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>634</th>\n      <td>表示这道题不理解</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>927</th>\n      <td>刚刚在网课上看到的</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>693</th>\n      <td>不过你是怎么倒过来了一下</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>571</th>\n      <td>直接洛必达一下就解决了啊</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1880</th>\n      <td>向老师求助请老师帮忙解答一下这三道题</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1631</th>\n      <td>咋做呢，老师</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>984</th>\n      <td>因为是任意给定的E</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1289</th>\n      <td>老师，这些是为什么啊求解，</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv('./data_lite_all.csv')\n",
    "df = pd.read_csv('../data_cleaned_1.csv')\n",
    "df=df[['text','label']]\n",
    "print(\"数据总量: %d .\" % len(df))\n",
    "df.sample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\WINDOWS\\TEMP\\jieba.cache\n",
      "Loading model cost 0.491 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                            text  label  \\\n0                                 为什么无界函数不一定为无穷大      0   \n1  判断，处有无定义，若，处有定义，但是，不存在，即，不存在或，不存在，若，处有定义存在，但是      0   \n2                                             谢谢      0   \n3                                     先开上面三次方再求导      0   \n4  求解答这里是讲收敛数列的有界性，为什么举例是这个例子这个数列并不收敛呀这个数列不是发散的吗      0   \n\n                                     clean_text  \\\n0                                为什么无界函数不一定为无穷大   \n1            判断处有无定义若处有定义但是不存在即不存在或不存在若处有定义存在但是   \n2                                            谢谢   \n3                                    先开上面三次方再求导   \n4  求解答这里是讲收敛数列的有界性为什么举例是这个例子这个数列并不收敛呀这个数列不是发散的吗   \n\n                                            cut_text  \n0                     [为什么, 无, 界, 函数, 不, 一定, 为, 无穷大]  \n1  [判断, 处, 有无, 定义, 若处, 有, 定义, 但是, 不, 存在, 即, 不, 存在...  \n2                                               [谢谢]  \n3                               [先开, 上面, 三次方, 再, 求导]  \n4  [求, 解答, 这里, 是, 讲, 收敛, 数列, 的, 有界性, 为什么, 举例, 是, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>clean_text</th>\n      <th>cut_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>为什么无界函数不一定为无穷大</td>\n      <td>0</td>\n      <td>为什么无界函数不一定为无穷大</td>\n      <td>[为什么, 无, 界, 函数, 不, 一定, 为, 无穷大]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>判断，处有无定义，若，处有定义，但是，不存在，即，不存在或，不存在，若，处有定义存在，但是</td>\n      <td>0</td>\n      <td>判断处有无定义若处有定义但是不存在即不存在或不存在若处有定义存在但是</td>\n      <td>[判断, 处, 有无, 定义, 若处, 有, 定义, 但是, 不, 存在, 即, 不, 存在...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>谢谢</td>\n      <td>0</td>\n      <td>谢谢</td>\n      <td>[谢谢]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>先开上面三次方再求导</td>\n      <td>0</td>\n      <td>先开上面三次方再求导</td>\n      <td>[先开, 上面, 三次方, 再, 求导]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>求解答这里是讲收敛数列的有界性，为什么举例是这个例子这个数列并不收敛呀这个数列不是发散的吗</td>\n      <td>0</td>\n      <td>求解答这里是讲收敛数列的有界性为什么举例是这个例子这个数列并不收敛呀这个数列不是发散的吗</td>\n      <td>[求, 解答, 这里, 是, 讲, 收敛, 数列, 的, 有界性, 为什么, 举例, 是, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba as jb\n",
    "#删除除字母,数字，汉字以外的所有符号\n",
    "df['clean_text'] = df['text'].apply(remove_punctuation)\n",
    "\n",
    "#分词，并过滤停用词\n",
    "# df['cut_review'] = df['clean_review'].apply(lambda x: \" \".join([w for w in list(jb.cut(x)) if w not in stopwords]))\n",
    "df['cut_text'] = df['clean_text'].apply(lambda x: [w for w in list(jb.cut(x)) if w not in stopwords])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "285     ([老师, 这个, 条件, 是, 么, 意思, 是, 确保, gx, 的, 极限, 为, u...\n1076                                ([这题, 怎么, 做, 啊], [2])\n164     ([如果, 我, 是, 你, 对于, 别人, 的, 这个, 问题, 我会, 回答, 分, X...\n56                                            ([谢谢], [0])\n1461                           ([为什么, 不能, 这么, 做, 呢], [2])\n1084                               ([第十一, 题, 怎么, 写], [2])\n156                        ([这道题, 有无, 更, 机智, 的, 做法], [0])\n1433                         ([咋, 做, 还是, 没看, 懂, 求教], [2])\n1230                                 ([那, 怎么, 求, 呢], [2])\n945     ([用, 放缩, 当, x, 趋于, 正, 无穷, 时, sinxxxx, 埃普, 西隆],...\ndtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "#创建标签化文档\n",
    "train_tagged = df.apply(\n",
    "    lambda r: TaggedDocument(words=r['cut_text'], tags=[r['label']]), axis=1)\n",
    "train_tagged.sample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 1999668.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "# model_dbow = Doc2Vec(dm=1, vector_size=300, negative=5, hs=0, min_count=2, sample = 1e-3, workers=cores, window=1)  # 0 &.7\n",
    "model_dbow = Doc2Vec(dm=1, vector_size=512, negative=5, hs=0, min_count=2, sample = 1e-3, workers=cores, window=1)  # 0 &.7\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:10<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import utils\n",
    "# for epoch in tqdm(range(10)):\n",
    "#     model_dbow.train(train_tagged.values, total_examples=len(train_tagged.values), epochs=30)\n",
    "for epoch in tqdm(range(1000)):\n",
    "    # model_dbow.train(utils.shuffle([x for x in train_tagged.values]), total_examples=len(train_tagged.values), epochs=10)\n",
    "    model_dbow.train(utils.shuffle([x for x in train_tagged.values]), total_examples=len(train_tagged.values), epochs=10)\n",
    "    # model_dbow.train(train_tagged.values, total_examples=len(train_tagged.values), epochs=7)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    # model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, regressors\n",
    "\n",
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(2000, 2000)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train = list(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_train = X_train[:3664]\n",
    "y_train = y_train[:3664]\n",
    "len(y_train),len(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "total = np.concatenate((X_train, y_train.reshape(len(y_train), 1)), axis=1)\n",
    "tp = pd.DataFrame(total)\n",
    "tp.to_csv(\"../d2v_cleaned_2000.csv\", header=None, index=None)\n",
    "# tp.to_csv(\"d2v.csv\", header=None, index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 513)\n",
      "0 0.01 0.3225\n",
      "1 0.011233240329780276 0.3225\n",
      "2 0.012618568830660204 0.335\n",
      "3 0.014174741629268055 0.3425\n",
      "4 0.015922827933410922 0.34\n",
      "5 0.01788649529057435 0.345\n",
      "6 0.02009233002565047 0.355\n",
      "7 0.022570197196339202 0.3575\n",
      "8 0.025353644939701114 0.3525\n",
      "9 0.02848035868435802 0.345\n",
      "10 0.03199267137797385 0.35\n",
      "11 0.03593813663804628 0.35\n",
      "12 0.040370172585965536 0.3525\n",
      "13 0.04534878508128582 0.355\n",
      "14 0.0509413801481638 0.345\n",
      "15 0.05722367659350217 0.35\n",
      "16 0.06428073117284319 0.3475\n",
      "17 0.07220809018385464 0.3425\n",
      "18 0.08111308307896872 0.345\n",
      "19 0.09111627561154892 0.34\n",
      "20 0.10235310218990264 0.3475\n",
      "21 0.11497569953977356 0.3525\n",
      "22 0.1291549665014884 0.3525\n",
      "23 0.14508287784959395 0.3425\n",
      "24 0.16297508346206444 0.34\n",
      "25 0.18307382802953678 0.335\n",
      "26 0.20565123083486514 0.3325\n",
      "27 0.23101297000831592 0.3325\n",
      "28 0.25950242113997357 0.335\n",
      "29 0.2915053062825176 0.335\n",
      "30 0.32745491628777285 0.3325\n",
      "31 0.36783797718286326 0.3325\n",
      "32 0.41320124001153363 0.33\n",
      "33 0.464158883361278 0.3325\n",
      "34 0.5214008287999684 0.3325\n",
      "35 0.5857020818056667 0.335\n",
      "36 0.6579332246575679 0.335\n",
      "37 0.739072203352578 0.335\n",
      "38 0.8302175681319743 0.34\n",
      "39 0.9326033468832199 0.3425\n",
      "40 1.0476157527896652 0.335\n",
      "41 1.1768119524349978 0.34\n",
      "42 1.3219411484660286 0.33\n",
      "43 1.484968262254465 0.3375\n",
      "44 1.6681005372000592 0.335\n",
      "45 1.873817422860383 0.3275\n",
      "46 2.1049041445120196 0.3325\n",
      "47 2.364489412645407 0.33\n",
      "48 2.656087782946687 0.3175\n",
      "49 2.9836472402833403 0.305\n",
      "50 3.351602650938841 0.33\n",
      "51 3.7649358067924674 0.315\n",
      "52 4.229242874389499 0.3175\n",
      "53 4.750810162102798 0.3175\n",
      "54 5.336699231206307 0.3125\n",
      "55 5.994842503189409 0.2925\n",
      "56 6.7341506577508214 0.29\n",
      "57 7.56463327554629 0.285\n",
      "58 8.497534359086439 0.2875\n",
      "59 9.545484566618338 0.2925\n",
      "60 10.722672220103231 0.2875\n",
      "61 12.045035402587823 0.2875\n",
      "62 13.530477745798061 0.2875\n",
      "63 15.199110829529332 0.2875\n",
      "64 17.073526474706906 0.2875\n",
      "65 19.179102616724887 0.285\n",
      "66 21.544346900318846 0.285\n",
      "67 24.20128264794381 0.285\n",
      "68 27.1858824273294 0.285\n",
      "69 30.538555088334157 0.285\n",
      "70 34.30469286314919 0.285\n",
      "71 38.53528593710527 0.285\n",
      "72 43.28761281083057 0.285\n",
      "73 48.62601580065353 0.285\n",
      "74 54.62277217684343 0.285\n",
      "75 61.35907273413169 0.285\n",
      "76 68.92612104349695 0.285\n",
      "77 77.4263682681127 0.285\n",
      "78 86.97490026177834 0.285\n",
      "79 97.70099572992247 0.285\n",
      "80 109.74987654930568 0.285\n",
      "81 123.28467394420659 0.285\n",
      "82 138.48863713938718 0.285\n",
      "83 155.56761439304722 0.285\n",
      "84 174.7528400007683 0.285\n",
      "85 196.30406500402725 0.285\n",
      "86 220.51307399030458 0.285\n",
      "87 247.70763559917089 0.285\n",
      "88 278.2559402207126 0.285\n",
      "89 312.5715849688235 0.285\n",
      "90 351.11917342151276 0.285\n",
      "91 394.4206059437656 0.285\n",
      "92 443.06214575838777 0.285\n",
      "93 497.7023564332114 0.285\n",
      "94 559.0810182512223 0.285\n",
      "95 628.0291441834247 0.285\n",
      "96 705.4802310718645 0.285\n",
      "97 792.482898353917 0.285\n",
      "98 890.2150854450392 0.285\n",
      "99 1000.0 0.285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.3376    0.4690    0.3926       113\n",
      "         1.0     0.3235    0.2418    0.2767        91\n",
      "         2.0     0.3053    0.2929    0.2990        99\n",
      "         3.0     0.3375    0.2784    0.3051        97\n",
      "\n",
      "    accuracy                         0.3275       400\n",
      "   macro avg     0.3260    0.3205    0.3183       400\n",
      "weighted avg     0.3264    0.3275    0.3218       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 后端网络LPA分类\n",
    "import numpy as np\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "data = np.concatenate((X_train, y_train.reshape(len(y_train), 1)), axis=1)\n",
    "print(data.shape)\n",
    "data = data[:2000]\n",
    "X = data[:, : -1]  # 输入\n",
    "y = data[:, -1]  # 输出\n",
    "rng = np.random.RandomState(5)  # 随机数种子\n",
    "indices = np.arange(len(data))\n",
    "rng.shuffle(indices)  # 将索引打乱\n",
    "# print(X[0])\n",
    "\n",
    "n_total_samples = len(y)\n",
    "# n_labeled_points = 2795  # 已标记/样本数\n",
    "n_labeled_points = 1600  # 已标记样本数1260 1440\n",
    "\n",
    "unlabeled_set = indices[n_labeled_points:]\n",
    "\n",
    "y_train = np.copy(y)\n",
    "y_train[unlabeled_set] = -1  # 未标记样本集y设为-1\n",
    "\n",
    "num=100\n",
    "accuracy = []\n",
    "gammas = np.logspace(-2, 3, num=num)\n",
    "# gammas = range(100)\n",
    "score_in = 0\n",
    "for i in range(num):\n",
    "    lp_model = LabelSpreading(gamma=gammas[i], max_iter=30, kernel='rbf')\n",
    "    lp_model.fit(X, y_train)\n",
    "    predicted_labels = lp_model.transduction_[unlabeled_set]\n",
    "    true_labels = y[unlabeled_set]\n",
    "    score = accuracy_score(true_labels, predicted_labels)\n",
    "    # if score > score_in:\n",
    "    #     gamma_fin = gammas[i]\n",
    "    accuracy.append(score)\n",
    "    print(i,gammas[i],score)\n",
    "    # print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "\n",
    "# lp_model = LabelSpreading(gamma=0.24, max_iter=200, kernel='rbf')# bert2000\n",
    "lp_model = LabelSpreading(gamma=1.9, max_iter=200, kernel='rbf')# tf-idf2000\n",
    "# lp_model = LabelSpreading(kernel='knn', n_neighbors=9, max_iter=100)\n",
    "lp_model.fit(X, y_train)\n",
    "predicted_labels = lp_model.transduction_[unlabeled_set]\n",
    "true_labels = y[unlabeled_set]\n",
    "print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "\n",
    "\n",
    "\n",
    "# print(\n",
    "#     \"Label Spreading model: %d labeled & %d unlabeled points (%d total)\"\n",
    "#     % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples)\n",
    "# )\n",
    "# print(classification_report(true_labels, predicted_labels))\n",
    "# print(true_labels)\n",
    "# print(predicted_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-4.74266335e-02, -3.02468613e-03, -7.81949237e-02,\n        -4.59347805e-03, -7.25578982e-03,  1.51773486e-02,\n         1.53552219e-02,  6.67010918e-02, -3.50089818e-02,\n         2.71189213e-02,  4.79480214e-02,  5.97529449e-02,\n         3.76101807e-02, -3.28362845e-02,  6.18035607e-02,\n        -4.01651636e-02,  3.46981920e-02,  4.31404868e-03,\n        -1.98813081e-02, -2.92790588e-02, -4.01854627e-02,\n         9.44314897e-03, -6.76565543e-02, -5.07358089e-02,\n        -3.45638879e-02,  2.20429190e-02, -1.13179915e-01,\n        -1.29152248e-02,  3.49431299e-03,  2.92934328e-02,\n         7.60602504e-02,  4.07384746e-02, -7.12106675e-02,\n         1.30992448e-02,  6.23914637e-02,  2.90053822e-02,\n         3.26382183e-02, -5.06384969e-02,  3.88111956e-02,\n        -3.36839370e-02, -3.57929058e-02,  3.27311195e-02,\n        -6.46381602e-02, -8.98270682e-02, -7.02664070e-03,\n         5.33068776e-02,  4.42453846e-02,  1.14518432e-02,\n         6.29458576e-02, -3.77650708e-02,  0.00000000e+00],\n       [-1.46979287e-01, -8.59602913e-02, -2.83165395e-01,\n         7.56181730e-03,  4.77358475e-02,  3.79279107e-02,\n        -1.57332281e-03,  1.15486428e-01, -1.51283979e-01,\n         1.00021221e-01,  1.57535151e-01,  8.69788975e-02,\n         7.84894675e-02,  1.76992156e-02,  5.64219840e-02,\n        -9.28711444e-02,  1.49790540e-01,  8.35517123e-02,\n        -2.61752419e-02, -1.43530160e-01, -1.37698129e-01,\n        -2.42909454e-02, -4.59192730e-02, -1.99510708e-01,\n        -8.04741979e-02, -5.35361506e-02, -2.16542259e-01,\n        -5.85222133e-02,  7.13508874e-02,  6.14907518e-02,\n         2.19867453e-01,  8.88024718e-02, -2.23861247e-01,\n         6.29292801e-02,  1.96725816e-01,  7.07669631e-02,\n         1.01919651e-01, -6.74156025e-02,  2.14822635e-01,\n        -1.00742653e-01, -1.68955252e-01,  6.02123886e-02,\n        -2.71073699e-01, -2.13747278e-01, -4.80876714e-02,\n         1.15037747e-01,  2.70273332e-02,  1.40915457e-02,\n         7.29797781e-02, -1.18461773e-01,  0.00000000e+00],\n       [-1.77197922e-02, -4.39634733e-03, -1.39702782e-02,\n        -3.73666151e-03,  7.67347123e-03, -1.47139495e-02,\n        -4.56888834e-03,  6.60337042e-03, -6.18916424e-03,\n         1.16596213e-02,  2.29286458e-02,  4.83898912e-03,\n        -1.06068584e-03, -9.63000581e-04,  1.76942237e-02,\n        -3.04862740e-03,  3.48111987e-03,  9.55651514e-03,\n        -4.61505307e-03, -1.09575549e-03, -9.41955857e-03,\n         5.13867382e-03, -9.97662917e-03, -5.86246327e-03,\n         9.85487457e-03, -9.29410476e-03, -3.57278325e-02,\n        -1.84478816e-02,  8.14861967e-04,  3.98980919e-03,\n         1.99655276e-02,  1.78978518e-02, -1.54025536e-02,\n        -1.63057563e-03,  7.79259950e-03,  4.50992538e-03,\n         1.35728680e-02, -1.06574921e-03, -1.13158207e-03,\n        -3.21369153e-03, -3.31921410e-03, -9.88192298e-03,\n         4.30507213e-03, -8.58496688e-03, -5.65159321e-03,\n        -2.95390282e-03,  8.50694254e-03, -4.08985233e-03,\n         2.88315397e-03, -1.20210797e-02,  0.00000000e+00],\n       [-2.03712340e-02, -3.10458262e-02, -8.39122608e-02,\n        -1.10209063e-02,  2.95482781e-02,  3.72998752e-02,\n        -4.76321317e-02, -4.79923673e-02, -1.64942909e-02,\n         6.05587736e-02, -2.69007776e-02,  1.22900987e-02,\n        -2.54933201e-02, -1.92247536e-02, -4.75010015e-02,\n        -2.85365433e-02,  4.85134721e-02,  7.30085745e-02,\n        -3.13944481e-02, -4.20363471e-02, -3.95119824e-02,\n         6.61195256e-04, -3.98553722e-02, -3.05106211e-02,\n        -1.25854472e-02, -2.97665633e-02, -4.88079004e-02,\n        -4.21876162e-02,  3.39370109e-02, -2.37347540e-02,\n         2.61935387e-02,  6.42280746e-03, -5.61813824e-02,\n        -2.49510277e-02,  5.15045114e-02, -1.48161864e-02,\n         2.94504706e-02, -2.84039360e-02,  2.24223677e-02,\n        -2.23852182e-03, -2.60983780e-02, -3.55051607e-02,\n        -5.59069254e-02, -9.15859640e-02, -4.04052390e-03,\n         1.67796649e-02, -3.02405600e-02, -3.26005667e-02,\n         4.18460630e-02, -4.22839224e-02,  0.00000000e+00],\n       [-5.36490902e-02, -7.79851452e-02, -1.67905450e-01,\n         1.36040477e-02, -2.08078511e-03, -7.92808272e-03,\n         1.69621296e-02,  7.06024691e-02, -1.95698634e-01,\n         1.06593892e-01,  1.06351040e-01,  4.76758070e-02,\n         4.85716271e-04,  2.09532548e-02,  5.80779016e-02,\n        -5.43411262e-02,  1.96993589e-01,  1.14891253e-01,\n         3.24405357e-02, -1.06504247e-01, -1.54459491e-01,\n        -6.50458410e-02, -4.11263146e-02, -1.17581926e-01,\n        -1.04152746e-01, -1.33141512e-02, -2.24147111e-01,\n        -6.94250986e-02,  1.06597789e-01,  2.92813350e-02,\n         1.38786823e-01,  8.57474953e-02, -1.53791025e-01,\n         5.22720367e-02,  2.07061082e-01,  2.52615735e-02,\n         8.66901726e-02, -8.57215226e-02,  2.03728914e-01,\n        -1.82274953e-01, -1.38049945e-01, -1.44071346e-02,\n        -2.69952953e-01, -2.32463509e-01, -5.53214289e-02,\n         1.08063810e-01,  4.32180427e-02, -1.14404308e-02,\n         1.00461051e-01, -5.88366315e-02,  0.00000000e+00],\n       [-5.56607135e-02, -4.57014181e-02, -1.53284863e-01,\n        -3.14644985e-02,  3.48134059e-03,  4.90868688e-02,\n        -4.08994108e-02,  7.37523362e-02, -1.21466286e-01,\n         4.38299477e-02,  5.19830687e-03,  4.49456312e-02,\n        -1.86978281e-02, -3.47880088e-02,  5.02986573e-02,\n        -8.41832161e-02,  5.92231825e-02,  1.99744608e-02,\n        -5.15166074e-02, -5.52757233e-02, -2.47064084e-02,\n         8.61723628e-03, -8.56953785e-02, -5.13062514e-02,\n        -5.09376489e-02,  1.43003957e-02, -1.09199256e-01,\n        -1.21695660e-01,  7.65959993e-02, -7.90157821e-03,\n         1.51017070e-01, -4.77722101e-03, -1.33725330e-01,\n        -3.17264870e-02,  6.23271912e-02,  3.08877416e-02,\n         8.38145614e-03, -6.30367221e-03,  1.14548177e-01,\n        -7.67869782e-03, -8.50953907e-02,  5.80646507e-02,\n        -1.13182604e-01, -1.71590850e-01, -7.51719028e-02,\n         8.75209942e-02, -4.86978516e-03, -5.01648802e-03,\n         1.07607313e-01, -4.89486195e-02,  0.00000000e+00],\n       [ 2.18467601e-02, -2.11708602e-02, -2.62585990e-02,\n         3.45729734e-03,  7.20810611e-03, -6.77058939e-03,\n        -1.19724916e-02, -1.50724296e-02, -3.89882922e-03,\n         5.21157635e-03,  1.91106834e-02,  2.24600453e-02,\n        -3.24463695e-02,  6.42512133e-03, -1.03463717e-02,\n         5.35991089e-03,  3.30161192e-02,  2.09051408e-02,\n         9.94579308e-03, -2.90729310e-02, -1.62925012e-02,\n        -8.88965279e-03,  4.12029156e-04, -2.13006977e-02,\n         1.21752378e-02, -4.50661592e-03, -9.89480875e-03,\n         7.80356722e-03,  1.10729430e-02,  1.58169456e-02,\n         1.00310929e-02,  2.35185940e-02,  4.98801190e-03,\n        -2.67323367e-02,  4.70179841e-02, -2.06491724e-02,\n         2.32792497e-02, -1.68286003e-02,  1.22839212e-02,\n        -2.16452517e-02, -2.14461647e-02, -1.23091694e-02,\n        -1.61900055e-02, -6.81021670e-03, -3.69428191e-03,\n        -1.35187595e-03,  6.99820695e-04, -3.45236948e-03,\n         1.21006751e-02, -2.10594181e-02,  0.00000000e+00],\n       [-6.65653124e-02, -4.38325144e-02, -1.12189800e-01,\n        -5.83098680e-02, -2.44099014e-02, -1.18861464e-03,\n         2.28087138e-02,  9.48066115e-02, -1.04927808e-01,\n         9.92438123e-02,  1.16718076e-01,  1.26193374e-01,\n         6.46212623e-02, -4.12574038e-02,  6.14537932e-02,\n        -8.47078785e-02,  9.34415162e-02,  5.39414883e-02,\n        -1.16833955e-01, -1.32547989e-01, -6.54051974e-02,\n        -3.85708138e-02, -6.06135391e-02, -1.03027925e-01,\n        -2.03038473e-02,  2.31663287e-02, -1.58164129e-01,\n        -9.02566537e-02,  6.08608313e-02, -3.02834925e-03,\n         1.78418264e-01,  1.00307211e-01, -1.93397105e-01,\n         3.95626463e-02,  1.64716437e-01,  1.15665272e-01,\n         3.46083455e-02, -1.17908612e-01,  1.44067451e-01,\n        -8.27210695e-02, -6.05281480e-02,  2.47615986e-02,\n        -1.52405515e-01, -1.99668169e-01, -6.40814379e-02,\n         1.44528240e-01,  9.80165154e-02, -8.95387866e-03,\n         6.97631165e-02, -3.59301008e-02,  0.00000000e+00],\n       [-4.12875041e-02, -1.12804502e-01, -2.71218628e-01,\n        -4.06795926e-02,  4.95213736e-03,  3.37978057e-03,\n        -6.83394149e-02,  4.05111052e-02, -1.72072142e-01,\n         1.17348686e-01,  1.27530813e-01,  9.61349458e-02,\n        -3.72825041e-02, -3.31146177e-03,  2.27688216e-02,\n        -1.04981050e-01,  1.55996174e-01,  1.41713545e-01,\n        -6.99954107e-02, -1.58792555e-01, -9.11190957e-02,\n         6.92080380e-03, -1.16883032e-01, -1.20291024e-01,\n        -8.59362856e-02, -8.38154554e-02, -2.15812996e-01,\n        -1.59248114e-01,  1.10673204e-01, -2.22982448e-02,\n         1.53384089e-01,  9.97255519e-02, -2.05385298e-01,\n        -2.98432447e-02,  1.82057187e-01,  1.88434422e-02,\n         7.52044618e-02, -9.32501405e-02,  1.45407155e-01,\n        -7.60572404e-02, -9.90890041e-02, -1.06261885e-02,\n        -2.27219105e-01, -2.63457924e-01, -2.97429855e-03,\n         9.38225240e-02,  2.60437932e-02, -1.29318852e-02,\n         1.66430712e-01, -9.77115706e-02,  0.00000000e+00],\n       [-7.27851540e-02, -2.13353224e-02, -2.16735169e-01,\n        -2.23422572e-02,  2.00658105e-04, -2.17771064e-02,\n         2.49343552e-02,  1.25003114e-01, -1.05016857e-01,\n         1.03647470e-01,  1.18698537e-01,  1.02207288e-01,\n         5.61417937e-02,  1.01612136e-02,  1.55288622e-01,\n        -1.28567889e-01,  1.18099906e-01,  4.53481935e-02,\n         5.05781593e-03, -6.03266396e-02, -6.10742345e-02,\n        -5.29200993e-02, -7.42225796e-02, -1.87320977e-01,\n        -1.71425231e-02, -5.05893193e-02, -2.04013243e-01,\n        -8.10823962e-02,  4.57905754e-02,  9.48562697e-02,\n         1.82195604e-01,  1.33809805e-01, -1.58197463e-01,\n         1.96501352e-02,  2.00672984e-01,  8.12377706e-02,\n         7.33270794e-02, -1.14464082e-01,  1.67556271e-01,\n        -3.33244987e-02, -1.00539640e-01,  1.02185383e-01,\n        -1.47084802e-01, -2.17293248e-01, -9.63683426e-03,\n         2.06688166e-01,  6.20005168e-02,  9.36943199e-03,\n         1.42672420e-01, -9.76924002e-02,  0.00000000e+00]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n",
    "data[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}